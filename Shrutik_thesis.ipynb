{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317f4db-308e-47e7-8e43-451c701e4744",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67b80bb-83cf-4032-83ed-1821f2e580b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import os\n",
    "#import pandas as pd\n",
    "#from tensorflow import keras\n",
    "#from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "class GlobalAttention(keras.layers.Layer):\n",
    "    \n",
    "    '''\n",
    "    Recevies two inputs:\n",
    "    1. A global representation (of some fixed dimension)\n",
    "    2. A sequence (of any length, and some fixed dimension)\n",
    "    The global representation is used to construct a global query that attends to all the positions in the sequence (independently\n",
    "    for any of the heads).\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_heads, d_key, d_value, **kwargs):\n",
    "        self.n_heads = n_heads\n",
    "        self.d_key = d_key\n",
    "        self.sqrt_d_key = np.sqrt(self.d_key)\n",
    "        self.d_value = d_value\n",
    "        self.d_output = n_heads * d_value\n",
    "        super(GlobalAttention, self).__init__(**kwargs)\n",
    "        \n",
    "    def compute_output_shape(self, input_shapes):\n",
    "        # input_shapes: (batch_size, d_global_input), (batch_size, length, d_seq_input)\n",
    "        (batch_size, _), _ = input_shapes\n",
    "        return (batch_size, self.d_output)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # input_shapes: (batch_size, d_global_input), (batch_size, length, d_seq_input)\n",
    "        (_, self.d_global_input), (_, _, self.d_seq_input) = input_shapes\n",
    "        # Wq: (n_heads, d_global_input, d_key)\n",
    "        self.Wq = self.add_weight(name = 'Wq', shape = (self.n_heads, self.d_global_input, self.d_key), \\\n",
    "                initializer = 'glorot_uniform', trainable = True)\n",
    "        # Wk: (n_heads, d_seq_input, d_key)\n",
    "        self.Wk = self.add_weight(name = 'Wk', shape = (self.n_heads, self.d_seq_input, self.d_key), \\\n",
    "                initializer = 'glorot_uniform', trainable = True)\n",
    "        # Wv: (n_heads, d_seq_input, d_value)\n",
    "        self.Wv = self.add_weight(name = 'Wv', shape = (self.n_heads, self.d_seq_input, self.d_value), \\\n",
    "                initializer = 'glorot_uniform', trainable = True)\n",
    "        super(GlobalAttention, self).build(input_shapes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "    \n",
    "        # X: (batch_size, d_global_input)\n",
    "        # S: (batch_size, length, d_seq_input)\n",
    "        X, S = inputs\n",
    "        _, length, _ = K.int_shape(S)\n",
    "    \n",
    "        # (batch_size, n_heads, length, d_value)\n",
    "        VS = K.permute_dimensions(keras.activations.gelu(K.dot(S, self.Wv)), (0, 2, 1, 3))\n",
    "        # (batch_size * n_heads, length, d_value)\n",
    "        VS_batched_heads = K.reshape(VS, (-1, length, self.d_value))\n",
    "        \n",
    "        Z_batched_heads = self.calculate_attention(inputs)\n",
    "        # (batch_size * n_heads, d_value)\n",
    "        Y_batched_heads = K.batch_dot(Z_batched_heads, VS_batched_heads)\n",
    "        # (batch_size, n_heads * d_value)\n",
    "        Y = K.reshape(Y_batched_heads, (-1, self.d_output))\n",
    "        \n",
    "        return Y\n",
    "        \n",
    "    def calculate_attention(self, inputs):\n",
    "    \n",
    "        # X: (batch_size, d_global_input)\n",
    "        # S: (batch_size, length, d_seq_input)\n",
    "        X, S = inputs\n",
    "        _, length, _ = K.int_shape(S)\n",
    "                \n",
    "        # (batch_size, n_heads, d_key)\n",
    "        QX = K.tanh(K.dot(X, self.Wq))\n",
    "        # (batch_size * n_heads, d_key)\n",
    "        QX_batched_heads = K.reshape(QX, (-1, self.d_key))\n",
    "        \n",
    "        # (batch_size, n_heads, d_key, length)\n",
    "        KS = K.permute_dimensions(K.tanh(K.dot(S, self.Wk)), (0, 2, 3, 1))\n",
    "        # (batch_size * n_heads, d_key, length)\n",
    "        KS_batched_heads = K.reshape(KS, (-1, self.d_key, length))\n",
    "                \n",
    "        # (batch_size * n_heads, length)\n",
    "        Z_batched_heads = K.softmax(K.batch_dot(QX_batched_heads, KS_batched_heads) / self.sqrt_d_key)\n",
    "        return Z_batched_heads\n",
    "    \n",
    "def create_model(seq_len, vocab_size, n_annotations, d_hidden_seq = 128, d_hidden_global = 512, n_blocks = 6, n_heads = 4, \\\n",
    "         d_key = 64, conv_kernel_size = 9, wide_conv_dilation_rate = 5, activation = 'gelu'):\n",
    "    \n",
    "    '''\n",
    "    seq_len is required to create the model, but all the weights are independent of the length and can be re-used with\n",
    "    different lengths.\n",
    "    '''\n",
    "    \n",
    "    assert d_hidden_global % n_heads == 0\n",
    "    d_value = d_hidden_global // n_heads\n",
    "    \n",
    "    input_seq = keras.layers.Input(shape = (seq_len,), dtype = np.int32, name = 'input-seq')\n",
    "    input_annotations = keras.layers.Input(shape = (n_annotations,), dtype = np.float32, name = 'input-annotations')\n",
    "    \n",
    "    hidden_seq = keras.layers.Embedding(vocab_size, d_hidden_seq, name = 'embedding-seq-input')(input_seq)\n",
    "    hidden_global = keras.layers.Dense(d_hidden_global, activation = activation, name = 'dense-global-input')(input_annotations)\n",
    "    \n",
    "    for block_index in range(1, n_blocks + 1):\n",
    "        \n",
    "        seqed_global = keras.layers.Dense(d_hidden_seq, activation = activation, name = 'global-to-seq-dense-block%d' % block_index)(hidden_global)\n",
    "        seqed_global = keras.layers.Reshape((1, d_hidden_seq), name = 'global-to-seq-reshape-block%d' % block_index)(seqed_global)\n",
    "        \n",
    "        narrow_conv_seq = keras.layers.Conv1D(filters = d_hidden_seq, kernel_size = conv_kernel_size, strides = 1, \\\n",
    "                padding = 'same', dilation_rate = 1, activation = activation, name = 'narrow-conv-block%d' % block_index)(hidden_seq)\n",
    "        wide_conv_seq = keras.layers.Conv1D(filters = d_hidden_seq, kernel_size = conv_kernel_size, strides = 1, \\\n",
    "                padding = 'same', dilation_rate = wide_conv_dilation_rate, activation = activation, name = 'wide-conv-block%d' % \\\n",
    "                block_index)(hidden_seq)\n",
    "        \n",
    "        hidden_seq = keras.layers.Add(name = 'seq-merge1-block%d' % block_index)([hidden_seq, seqed_global, narrow_conv_seq, wide_conv_seq])\n",
    "        hidden_seq = keras.layers.LayerNormalization(name = 'seq-merge1-norm-block%d' % block_index)(hidden_seq)\n",
    "        \n",
    "        dense_seq = keras.layers.Dense(d_hidden_seq, activation = activation, name = 'seq-dense-block%d' % block_index)(hidden_seq)\n",
    "        hidden_seq = keras.layers.Add(name = 'seq-merge2-block%d' % block_index)([hidden_seq, dense_seq])\n",
    "        hidden_seq = keras.layers.LayerNormalization(name = 'seq-merge2-norm-block%d' % block_index)(hidden_seq)\n",
    "        \n",
    "        dense_global = keras.layers.Dense(d_hidden_global, activation = activation, name = 'global-dense1-block%d' % block_index)(hidden_global)\n",
    "        attention = GlobalAttention(n_heads, d_key, d_value, name = 'global-attention-block%d' % block_index)([hidden_global, hidden_seq])\n",
    "        hidden_global = keras.layers.Add(name = 'global-merge1-block%d' % block_index)([hidden_global, dense_global, attention])\n",
    "        hidden_global = keras.layers.LayerNormalization(name = 'global-merge1-norm-block%d' % block_index)(hidden_global)\n",
    "        \n",
    "        dense_global = keras.layers.Dense(d_hidden_global, activation = activation, name = 'global-dense2-block%d' % block_index)(hidden_global)\n",
    "        hidden_global = keras.layers.Add(name = 'global-merge2-block%d' % block_index)([hidden_global, dense_global])\n",
    "        hidden_global = keras.layers.LayerNormalization(name = 'global-merge2-norm-block%d' % block_index)(hidden_global)\n",
    "        \n",
    "    output_seq = keras.layers.Dense(vocab_size, activation = 'softmax', name = 'output-seq')(hidden_seq)\n",
    "    output_annotations = keras.layers.Dense(n_annotations, activation = 'sigmoid', name = 'output-annotations')(hidden_global)\n",
    "\n",
    "    return keras.models.Model(inputs = [input_seq, input_annotations], outputs = [output_seq, output_annotations])\n",
    "    \n",
    "def get_model_with_hidden_layers_as_outputs(model):\n",
    "    \n",
    "    _, seq_len, _ = model.outputs[0].shape\n",
    "    \n",
    "    seq_layers = [layer.output for layer in model.layers if len(layer.output.shape) == 3 and \\\n",
    "            tuple(layer.output.shape)[:2] == (None, seq_len) and (layer.name in ['input-seq-encoding', 'dense-seq-input', 'output-seq'] or \\\n",
    "            isinstance(layer, keras.layers.LayerNormalization))]\n",
    "    global_layers = [layer.output for layer in model.layers if len(layer.output.shape) == 2 and (layer.name in ['input_annotations', \\\n",
    "            'dense-global-input', 'output-annotations'] or isinstance(layer, keras.layers.LayerNormalization))]\n",
    "    \n",
    "    concatenated_seq_output = keras.layers.Concatenate(name = 'all-seq-layers')(seq_layers)\n",
    "    concatenated_global_output = keras.layers.Concatenate(name = 'all-global-layers')(global_layers)\n",
    "    \n",
    "    return keras.models.Model(inputs = model.inputs, outputs = [concatenated_seq_output, concatenated_global_output])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bff81e0-f5c7-4f4d-92ad-ead542d39eb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'typeDict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mh5py\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     16\u001b[0m DEFAULT_EPISODE_SETTINGS \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;66;03m# seq_len, batch_size\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     (\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m),\n\u001b[1;32m     19\u001b[0m     (\u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m64\u001b[39m),\n\u001b[1;32m     20\u001b[0m     (\u001b[38;5;241m1024\u001b[39m, \u001b[38;5;241m32\u001b[39m),\n\u001b[1;32m     21\u001b[0m ]\n",
      "File \u001b[0;32m~/Desktop/SEM 2/Thesis/Data/protein_bert-master/venv/shrutik_thesis/lib/python3.8/site-packages/h5py/__init__.py:46\u001b[0m\n\u001b[1;32m     37\u001b[0m     _warn((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh5py is running against HDF5 \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m when it was built against \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis may cause problems\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_version_tuple),\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39mversion\u001b[38;5;241m.\u001b[39mhdf5_built_version_tuple)\n\u001b[1;32m     41\u001b[0m     ))\n\u001b[1;32m     44\u001b[0m _errors\u001b[38;5;241m.\u001b[39msilence_errors()\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_conv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_converters \u001b[38;5;28;01mas\u001b[39;00m _register_converters\n\u001b[1;32m     47\u001b[0m _register_converters()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mh5z\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register_lzf\n",
      "File \u001b[0;32mh5py/h5t.pxd:14\u001b[0m, in \u001b[0;36minit h5py._conv\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5t.pyx:293\u001b[0m, in \u001b[0;36minit h5py.h5t\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/SEM 2/Thesis/Data/protein_bert-master/venv/shrutik_thesis/lib/python3.8/site-packages/numpy/__init__.py:320\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'typeDict'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "DEFAULT_EPISODE_SETTINGS = [\n",
    "    # seq_len, batch_size\n",
    "    (128, 128),\n",
    "    (512, 64),\n",
    "    (1024, 32),\n",
    "]\n",
    "\n",
    "def run_pretraining(create_model_function, epoch_generator, h5_dataset_file_path, create_model_kwargs = {}, optimizer_class = keras.optimizers.Adam, lr = 2e-04, \\\n",
    "        other_optimizer_kwargs = {}, annots_loss_weight = 1, autosave_manager = None, weights_dir = None, resume_from = None, n_epochs = None, fit_callbacks = []):\n",
    "\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    with h5py.File(h5_dataset_file_path, 'r') as h5f:\n",
    "        n_annotations = len(h5f['included_annotations'])\n",
    "    \n",
    "    model_generator = PretrainingModelGenerator(create_model_function, n_annotations, create_model_kwargs = create_model_kwargs, optimizer_class = optimizer_class, lr = lr, \\\n",
    "            other_optimizer_kwargs = other_optimizer_kwargs, annots_loss_weight = annots_loss_weight)\n",
    "    model_trainer = ModelTrainer(model_generator, epoch_generator, autosave_manager = autosave_manager, weights_dir = weights_dir, fit_callbacks = fit_callbacks)\n",
    "\n",
    "    with h5py.File(h5_dataset_file_path, 'r') as h5f:\n",
    "        model_trainer.setup(DatasetHandler(h5f), resume_from = resume_from)\n",
    "        model_trainer.train(n_epochs = n_epochs)\n",
    "        \n",
    "    return model_trainer\n",
    "    \n",
    "class ModelTrainer:\n",
    "    \n",
    "    def __init__(self, model_generator, epoch_generator, autosave_manager = None, weights_dir = None, fit_callbacks = []):\n",
    "        \n",
    "        self.model_generator = model_generator\n",
    "        self.epoch_generator = epoch_generator\n",
    "        self.autosave_manager = autosave_manager\n",
    "        self.weights_dir = weights_dir\n",
    "        self.fit_callbacks = fit_callbacks\n",
    "        \n",
    "        if self.autosave_manager is not None:\n",
    "            self.autosave_manager.n_annotations = self.model_generator.n_annotations\n",
    "        \n",
    "    def setup(self, dataset_handler, resume_from = None):\n",
    "        \n",
    "        if resume_from is None:\n",
    "            self.current_epoch_index = 0\n",
    "            start_sample_index = 0\n",
    "            resumed_weights_file_path = None\n",
    "        else:\n",
    "            self.current_epoch_index, start_sample_index = resume_from\n",
    "            self.current_epoch_index += 1\n",
    "            resumed_weights_file_path = os.path.join(self.weights_dir, 'epoch_%d_sample_%d.pkl' % resume_from)\n",
    "        \n",
    "        starting_episode = self.epoch_generator.setup(dataset_handler, start_sample_index)\n",
    "        self.model_generator.dummy_epoch = self.epoch_generator.create_dummpy_epoch()[:2]\n",
    "        log('Starting with episode with seq_len = %d.' % starting_episode.seq_len)\n",
    "        \n",
    "        if resumed_weights_file_path is not None:\n",
    "            with open(resumed_weights_file_path, 'rb') as f:\n",
    "                n_annotations, self.model_generator.model_weights, self.model_generator.optimizer_weights = pickle.load(f)\n",
    "                assert n_annotations == self.model_generator.n_annotations\n",
    "                log('Loaded weights from %s.' % resumed_weights_file_path)\n",
    "        \n",
    "        self.model = self.model_generator.create_model(starting_episode.seq_len)\n",
    "        self.model.summary()\n",
    "                    \n",
    "    def train(self, n_epochs = None, autosave = True):\n",
    "        for _ in (itertools.count() if n_epochs is None else range(n_epochs)):\n",
    "            self.train_next_epoch(autosave = autosave)\n",
    "        \n",
    "    def train_next_epoch(self, autosave = True):\n",
    "    \n",
    "        changed_episode, episode = self.epoch_generator.determine_episode_and_ready_next_epoch()\n",
    "        \n",
    "        if changed_episode:\n",
    "            log('Starting a new episode with seq_len = %d.' % episode.seq_len)\n",
    "            self.model_generator.dummy_epoch = self.epoch_generator.create_dummpy_epoch()[:2]\n",
    "            self.model_generator.update_state(self.model)\n",
    "            self.model = self.model_generator.create_model(episode.seq_len)\n",
    "        \n",
    "        X, Y, sample_weights = self.epoch_generator.create_next_epoch()\n",
    "        log('Epoch %d (current sample %d):' % (self.current_epoch_index, self.epoch_generator.current_sample_index))\n",
    "        self.model.fit(X, Y, sample_weight = sample_weights, batch_size = episode.batch_size, callbacks = self.fit_callbacks)\n",
    "        \n",
    "        if autosave and self.autosave_manager is not None:\n",
    "            self.autosave_manager.on_epoch_end(self.model, self.current_epoch_index, self.epoch_generator.current_sample_index)\n",
    "            \n",
    "        self.current_epoch_index += 1\n",
    "\n",
    "class EpochGenerator:\n",
    "    \n",
    "    def __init__(self, n_batches_per_epoch = 100, p_seq_noise = 0.05, p_no_input_annot = 0.5, p_annot_noise_positive = 0.25, \\\n",
    "            p_annot_noise_negative = 1e-04, load_chunk_size = 100000, min_time_per_episode = timedelta(minutes = 15), \\\n",
    "            episode_settings = DEFAULT_EPISODE_SETTINGS):\n",
    "        \n",
    "        self.n_batches_per_epoch = n_batches_per_epoch\n",
    "        self.p_seq_noise = p_seq_noise\n",
    "        self.p_no_input_annot = p_no_input_annot\n",
    "        self.p_annot_noise_positive = p_annot_noise_positive\n",
    "        self.p_annot_noise_negative = p_annot_noise_negative\n",
    "        self.load_chunk_size = load_chunk_size\n",
    "        self.min_time_per_episode = min_time_per_episode\n",
    "        \n",
    "        self.episode_managers = [EpisodeDataManager(seq_len, batch_size, self.n_batches_per_epoch) for seq_len, batch_size in \\\n",
    "                episode_settings]\n",
    "        self.episode_seq_lens = np.array([episode_manager.seq_len for episode_manager in self.episode_managers])\n",
    "        \n",
    "    def setup(self, dataset_handler, start_sample_index = 0):\n",
    "        self.dataset_handler = dataset_handler\n",
    "        self.current_sample_index = start_sample_index % self.dataset_handler.total_size\n",
    "        self._load_chunk()\n",
    "        self._select_new_episode()\n",
    "        return self._current_episode\n",
    "    \n",
    "    def determine_episode_and_ready_next_epoch(self):\n",
    "        \n",
    "        if self._episode_selection_time + self.min_time_per_episode <= datetime.now():\n",
    "            old_episode = self._current_episode\n",
    "            self._select_new_episode()\n",
    "            changed_episode = (self._current_episode is not old_episode)\n",
    "        else:\n",
    "            changed_episode = False\n",
    "            \n",
    "        while not self._current_episode.is_epoch_ready():\n",
    "            self._load_chunk()\n",
    "\n",
    "        return changed_episode, self._current_episode\n",
    "        \n",
    "    def create_next_epoch(self):\n",
    "        return self._encode_epoch(*self.create_next_epoch_Y())\n",
    "        \n",
    "    def create_dummpy_epoch(self, size = 1):\n",
    "        return self._encode_epoch(*self.create_next_dummy_epoch_Y(size))\n",
    "        \n",
    "    def create_next_epoch_Y(self):\n",
    "        assert self._current_episode.is_epoch_ready()\n",
    "        return self._current_episode.encode_next_epoch()\n",
    "    \n",
    "    def create_next_dummy_epoch_Y(self, size = 1):\n",
    "        \n",
    "        while not self._current_episode.is_epoch_ready(size):\n",
    "            self._load_chunk()\n",
    "            \n",
    "        return self._current_episode.encode_dummy_epoch(size)\n",
    "    \n",
    "    def _select_new_episode(self):\n",
    "        self._current_episode = max(self.episode_managers, key = lambda episode_manager: len(episode_manager.sample_cache))\n",
    "        self._episode_selection_time = datetime.now()\n",
    "            \n",
    "    def _load_chunk(self):\n",
    "        \n",
    "        chunk_sample_cache = self.dataset_handler[self.current_sample_index:(self.current_sample_index + self.load_chunk_size)]\n",
    "        self.current_sample_index += self.load_chunk_size\n",
    "        \n",
    "        if self.current_sample_index >= self.dataset_handler.total_size:\n",
    "            self.current_sample_index = 0\n",
    "            \n",
    "        self._assign_samples(chunk_sample_cache)\n",
    "        \n",
    "    def _assign_samples(self, sample_cache):\n",
    "        \n",
    "        seq_lens = np.array(list(map(len, sample_cache.seqs))) + ADDED_TOKENS_PER_SEQ\n",
    "        assigned_episode_indices = self._select_episodes_to_assign(seq_lens)\n",
    "        \n",
    "        for episode_manager_index, episode_manager in enumerate(self.episode_managers):\n",
    "            sample_indices_for_episode, = np.where(assigned_episode_indices == episode_manager_index)\n",
    "            episode_manager.sample_cache.extend(sample_cache.slice_indices(sample_indices_for_episode))\n",
    "        \n",
    "    def _select_episodes_to_assign(self, seq_lens, gamma = 1):\n",
    "        # The smaller the distance between a sample's sequence length to an episode's maximum sequence length, the higher the chance\n",
    "        # that it will be assigned to that episode.\n",
    "        samples_by_episodes_seq_len_ratio = seq_lens.reshape(-1, 1) / self.episode_seq_lens.reshape(1, -1)\n",
    "        samples_by_episodes_seq_len_symmetric_ratio = np.maximum(samples_by_episodes_seq_len_ratio, 1 / samples_by_episodes_seq_len_ratio)\n",
    "        raw_samples_by_episodes_probs = np.exp(-gamma * samples_by_episodes_seq_len_symmetric_ratio)\n",
    "        samples_by_episodes_probs = raw_samples_by_episodes_probs / raw_samples_by_episodes_probs.sum(axis = -1).reshape(-1, 1)\n",
    "        samples_by_episodes_cum_probs = samples_by_episodes_probs.cumsum(axis = -1)\n",
    "        assigned_episode_indices = (np.random.rand(len(seq_lens), 1) <= samples_by_episodes_cum_probs).argmax(axis = 1)\n",
    "        return assigned_episode_indices\n",
    "    \n",
    "    def _encode_epoch(self, encoded_seqs, encoded_annotation_masks):\n",
    "        \n",
    "        seqs_noise_mask = np.random.choice([True, False], encoded_seqs.shape, p = [1 - self.p_seq_noise, self.p_seq_noise])\n",
    "        random_seq_tokens = np.random.randint(0, n_tokens, encoded_seqs.shape)\n",
    "        noisy_encoded_seqs = np.where(seqs_noise_mask, encoded_seqs, random_seq_tokens)\n",
    "\n",
    "        noisy_annotations_when_positive = np.random.choice([True, False], encoded_annotation_masks.shape, \\\n",
    "                p = [1 - self.p_annot_noise_positive, self.p_annot_noise_positive])\n",
    "        noisy_annotations_when_negative = np.random.choice([True, False], encoded_annotation_masks.shape, \\\n",
    "                p = [self.p_annot_noise_negative, 1 - self.p_annot_noise_negative])\n",
    "        noisy_annotation_masks = np.where(encoded_annotation_masks, noisy_annotations_when_positive, \\\n",
    "                noisy_annotations_when_negative)\n",
    "        noisy_annotation_masks[np.random.choice([True, False], len(noisy_annotation_masks), p = [self.p_no_input_annot, \\\n",
    "                1 - self.p_no_input_annot]), :] = False\n",
    "\n",
    "        seq_weights = (encoded_seqs != additional_token_to_index['<PAD>']).astype(float)\n",
    "        # When a protein has no annotations at all, we don't know whether it's because such annotations don't exist or just not found,\n",
    "        # so it's safer to set the loss weight of those annotations to zero.\n",
    "        annotation_weights = encoded_annotation_masks.any(axis = -1).astype(float)\n",
    "        \n",
    "        X = [noisy_encoded_seqs, noisy_annotation_masks.astype(np.int8)]\n",
    "        Y = [np.expand_dims(encoded_seqs, axis = -1), encoded_annotation_masks.astype(np.int8)]\n",
    "        sample_weigths = [seq_weights, annotation_weights]\n",
    "        \n",
    "        return X, Y, sample_weigths\n",
    "\n",
    "class EpisodeDataManager:\n",
    "    \n",
    "    def __init__(self, seq_len, batch_size, n_batches_per_epoch):\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches_per_epoch = n_batches_per_epoch\n",
    "        self.epoch_size = self.n_batches_per_epoch * self.batch_size\n",
    "        self.sample_cache = SampleCache()\n",
    "        \n",
    "    def is_epoch_ready(self, n_required_samples = None):\n",
    "        return len(self.sample_cache) >= self._resolve_epoch_size(n_required_samples)\n",
    "    \n",
    "    def get_next_raw_epoch(self, size = None):\n",
    "        return self.sample_cache.pop(self._resolve_epoch_size(size))\n",
    "    \n",
    "    def peek_raw_epoch(self, size = None):\n",
    "        return self.sample_cache.slice_first(self._resolve_epoch_size(size))\n",
    "    \n",
    "    def encode_next_epoch(self, log_length_dist = True):\n",
    "        \n",
    "        seq_lengths, encoded_seqs, encoded_annotation_masks = self._encode_epoch(self.get_next_raw_epoch())\n",
    "        \n",
    "        if log_length_dist:\n",
    "            log('Epoch sequence length distribution (for seq_len = %d): %s' % (self.seq_len, \\\n",
    "                    ', '.join('%s: %s' % item for item in pd.Series(seq_lengths).describe().iteritems())))\n",
    "        \n",
    "        return encoded_seqs, encoded_annotation_masks\n",
    "    \n",
    "    def encode_dummy_epoch(self, size = 1):\n",
    "        seq_lengths, encoded_seqs, encoded_annotation_masks = self._encode_epoch(self.peek_raw_epoch(size))\n",
    "        return encoded_seqs, encoded_annotation_masks\n",
    "    \n",
    "    def _encode_epoch(self, epoch_sample_cache):\n",
    "        \n",
    "        pad_token_index = additional_token_to_index['<PAD>']\n",
    "        tokenized_seqs = list(map(tokenize_seq, epoch_sample_cache.seqs))\n",
    "        seq_lengths = np.array(list(map(len, tokenized_seqs)))\n",
    "        max_offsets = np.maximum(seq_lengths - self.seq_len, 0)\n",
    "        chosen_offsets = (np.random.rand(self.epoch_size) * (max_offsets + 1)).astype(int)\n",
    "        trimmed_tokenized_seqs = [seq_tokens[chosen_offset:(chosen_offset + self.seq_len)] for seq_tokens, chosen_offset in \\\n",
    "                zip(tokenized_seqs, chosen_offsets)]\n",
    "        encoded_seqs = np.array([seq_tokens + max(self.seq_len - len(seq_tokens), 0) * [pad_token_index] for seq_tokens in \\\n",
    "                trimmed_tokenized_seqs]).astype(np.int8)\n",
    "        \n",
    "        encoded_annotation_masks = np.concatenate([annotation_mask.reshape(1, -1) for annotation_mask in \\\n",
    "                epoch_sample_cache.annotation_masks], axis = 0).astype(bool)\n",
    "        \n",
    "        # We hide the annotations of test-set samples to avoid \"cheating\" on downstream fine-tuning tests. Note that by removing all of the annotations,\n",
    "        # EpochGenerator._encode_epoch will then set the annotation_weights for these records to 0, meaning they will not be part of the loss function.\n",
    "        encoded_annotation_masks[epoch_sample_cache.test_set_mask, :] = False\n",
    "        \n",
    "        return seq_lengths, encoded_seqs, encoded_annotation_masks\n",
    "    \n",
    "    def _resolve_epoch_size(self, size):\n",
    "        if size is None:\n",
    "            return self.epoch_size\n",
    "        else:\n",
    "            return size\n",
    "\n",
    "class DatasetHandler:\n",
    "    \n",
    "    def __init__(self, dataset_h5f):\n",
    "        self.dataset_h5f = dataset_h5f\n",
    "        self.total_size = len(dataset_h5f['seq_lengths'])\n",
    "        \n",
    "    def __getitem__(self, slicing):\n",
    "        return SampleCache(list(map(parse_seq, self.dataset_h5f['seqs'][slicing])), self.dataset_h5f['annotation_masks'][slicing], \\\n",
    "                self.dataset_h5f['test_set_mask'][slicing])\n",
    "\n",
    "class SampleCache:\n",
    "    \n",
    "    def __init__(self, seqs = [], annotation_masks = [], test_set_mask = []):\n",
    "        self.seqs = list(seqs)\n",
    "        self.annotation_masks = list(annotation_masks)\n",
    "        self.test_set_mask = list(test_set_mask)\n",
    "        \n",
    "    def extend(self, other_cache):\n",
    "        self.seqs.extend(other_cache.seqs)\n",
    "        self.annotation_masks.extend(other_cache.annotation_masks)\n",
    "        self.test_set_mask.extend(other_cache.test_set_mask)\n",
    "        \n",
    "    def pop(self, n):\n",
    "        popped_sample_cache = self.slice_first(n)\n",
    "        self.seqs = self.seqs[n:]\n",
    "        self.annotation_masks = self.annotation_masks[n:]\n",
    "        self.test_set_mask = self.test_set_mask[n:]\n",
    "        return popped_sample_cache\n",
    "    \n",
    "    def slice_first(self, n):\n",
    "        return SampleCache(self.seqs[:n], self.annotation_masks[:n], self.test_set_mask[:n])\n",
    "        \n",
    "    def slice_indices(self, indices):\n",
    "        return SampleCache([self.seqs[i] for i in indices], [self.annotation_masks[i] for i in indices], \\\n",
    "                [self.test_set_mask[i] for i in indices])\n",
    "    \n",
    "    def __len__(self):\n",
    "        assert len(self.seqs) == len(self.annotation_masks) == len(self.test_set_mask)\n",
    "        return len(self.seqs)\n",
    "        \n",
    "class AutoSaveManager:\n",
    "    \n",
    "    def __init__(self, directory, every_epochs_to_save = 10, every_saves_to_keep = 25):\n",
    "        self.directory = directory\n",
    "        self.every_epochs_to_save = every_epochs_to_save\n",
    "        self.every_saves_to_keep = every_saves_to_keep\n",
    "        self.last_saved_path_to_delete = None\n",
    "        self.n_saves = 0\n",
    "    \n",
    "    def on_epoch_end(self, model, epoch_index, sample_index):\n",
    "        \n",
    "        if epoch_index % self.every_epochs_to_save != 0:\n",
    "            return\n",
    "        \n",
    "        save_path = os.path.join(self.directory, 'epoch_%d_sample_%d.pkl' % (epoch_index, sample_index))\n",
    "        _save_model_state(model, self.n_annotations, save_path)\n",
    "        self.n_saves += 1\n",
    "        \n",
    "        if self.last_saved_path_to_delete is not None:\n",
    "            os.remove(self.last_saved_path_to_delete)\n",
    "            \n",
    "        if self.n_saves % self.every_saves_to_keep == 0:\n",
    "            self.last_saved_path_to_delete = None\n",
    "        else:\n",
    "            self.last_saved_path_to_delete = save_path\n",
    "\n",
    "def _save_model_state(model, n_annotations, path):\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump((n_annotations, model.get_weights(), model.optimizer.get_weights()), f)\n",
    "        \n",
    "\n",
    "class ModelGenerator:\n",
    "\n",
    "    def __init__(self, optimizer_class = keras.optimizers.Adam, lr = 2e-04, other_optimizer_kwargs = {}, model_weights = None, optimizer_weights = None):\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.lr = lr\n",
    "        self.other_optimizer_kwargs = other_optimizer_kwargs\n",
    "        self.model_weights = model_weights\n",
    "        self.optimizer_weights = optimizer_weights\n",
    "        \n",
    "    def train(self, encoded_train_set, encoded_valid_set, seq_len, batch_size, n_epochs, lr = None, callbacks = [], **create_model_kwargs):\n",
    "    \n",
    "        train_X, train_Y, train_sample_weigths = encoded_train_set\n",
    "        self.dummy_epoch = (_slice_arrays(train_X, slice(0, 1)), _slice_arrays(train_Y, slice(0, 1)))\n",
    "        model = self.create_model(seq_len, **create_model_kwargs)\n",
    "        \n",
    "        if lr is not None:\n",
    "            model.optimizer.lr = lr\n",
    "        \n",
    "        model.fit(train_X, train_Y, sample_weight = train_sample_weigths, batch_size = batch_size, epochs = n_epochs, validation_data = encoded_valid_set, \\\n",
    "                callbacks = callbacks)\n",
    "        self.update_state(model)\n",
    "        \n",
    "    # def update_state(self, model):\n",
    "    #     self.model_weights = copy_weights([w.numpy() for w in model.variables])\n",
    "    #     self.optimizer_weights = copy_weights([w.numpy() for w in model.optimizer.variables()])\n",
    "        \n",
    "    def update_state(self, model):\n",
    "        self.model_weights = copy_weights([w.numpy() for w in model.variables])\n",
    "        self.optimizer_weights = copy_weights([w.numpy() for w in model.optimizer.variables])\n",
    "        \n",
    "    def _init_weights(self, model):\n",
    "    \n",
    "        if self.optimizer_weights is not None:\n",
    "            # For some reason keras requires this strange little hack in order to properly initialize a new model's optimizer, so that\n",
    "            # the optimizer's weights can be reloaded from an existing state.\n",
    "            self._train_for_a_dummy_epoch(model)\n",
    "            \n",
    "        if self.model_weights is not None:\n",
    "            print(len(copy_weights(self.model_weights)))\n",
    "            model.set_weights(copy_weights(self.model_weights))\n",
    "        \n",
    "        if self.optimizer_weights is not None:\n",
    "            if len(self.optimizer_weights) == len(model.optimizer.variables()):\n",
    "                model.optimizer.set_weights(copy_weights(self.optimizer_weights))\n",
    "            else:\n",
    "                log('Incompatible number of optimizer weights - will not initialize them.')\n",
    "            \n",
    "    def _train_for_a_dummy_epoch(self, model):\n",
    "        X, Y = self.dummy_epoch\n",
    "        model.fit(X, Y, batch_size = 1, verbose = 0)\n",
    "        \n",
    "class PretrainingModelGenerator(ModelGenerator):\n",
    "\n",
    "    def __init__(self, create_model_function, n_annotations, create_model_kwargs = {}, optimizer_class = keras.optimizers.Adam, lr = 2e-04, other_optimizer_kwargs = {}, \\\n",
    "            annots_loss_weight = 1, model_weights = None, optimizer_weights = None):\n",
    "        \n",
    "        ModelGenerator.__init__(self, optimizer_class = optimizer_class, lr = lr, other_optimizer_kwargs = other_optimizer_kwargs, model_weights = model_weights, \\\n",
    "                optimizer_weights = optimizer_weights)\n",
    "        \n",
    "        self.create_model_function = create_model_function\n",
    "        self.n_annotations = n_annotations\n",
    "        self.create_model_kwargs = create_model_kwargs\n",
    "        self.annots_loss_weight = annots_loss_weight\n",
    "        \n",
    "    def create_model(self, seq_len, compile = True, init_weights = True):\n",
    "        \n",
    "        clear_session()\n",
    "        model = self.create_model_function(seq_len, n_tokens, self.n_annotations, **self.create_model_kwargs)\n",
    "        \n",
    "        if compile:\n",
    "            model.compile(optimizer =self.optimizer_class(learning_rate = self.lr, **self.other_optimizer_kwargs), loss = ['sparse_categorical_crossentropy', 'binary_crossentropy'], \\\n",
    "                    loss_weights = [1, self.annots_loss_weight])\n",
    "        \n",
    "        if init_weights:\n",
    "            self._init_weights(model)\n",
    "        \n",
    "        return model\n",
    "        \n",
    "class FinetuningModelGenerator(ModelGenerator):\n",
    "\n",
    "    def __init__(self, pretraining_model_generator, output_spec, pretraining_model_manipulation_function = None, dropout_rate = 0.5, optimizer_class = None, \\\n",
    "            lr = None, other_optimizer_kwargs = None, model_weights = None, optimizer_weights = None):\n",
    "        \n",
    "        if other_optimizer_kwargs is None:\n",
    "            if optimizer_class is None:\n",
    "                other_optimizer_kwargs = pretraining_model_generator.other_optimizer_kwargs\n",
    "            else:\n",
    "                other_optimizer_kwargs = {}\n",
    "        \n",
    "        if optimizer_class is None:\n",
    "            optimizer_class = pretraining_model_generator.optimizer_class\n",
    "            \n",
    "        if lr is None:\n",
    "            lr = pretraining_model_generator.lr\n",
    "            \n",
    "        ModelGenerator.__init__(self, optimizer_class = optimizer_class, lr = lr, other_optimizer_kwargs = other_optimizer_kwargs, model_weights = model_weights, \\\n",
    "                optimizer_weights = optimizer_weights)\n",
    "        \n",
    "        self.pretraining_model_generator = pretraining_model_generator\n",
    "        self.output_spec = output_spec\n",
    "        self.pretraining_model_manipulation_function = pretraining_model_manipulation_function\n",
    "        self.dropout_rate = dropout_rate\n",
    "                    \n",
    "    def create_model(self, seq_len, freeze_pretrained_layers = False):\n",
    "        \n",
    "        model = self.pretraining_model_generator.create_model(seq_len, compile = False, init_weights = (self.model_weights is None))\n",
    "            \n",
    "        if self.pretraining_model_manipulation_function is not None:\n",
    "            model = self.pretraining_model_manipulation_function(model)\n",
    "            \n",
    "        if freeze_pretrained_layers:\n",
    "            for layer in model.layers:\n",
    "                layer.trainable = False\n",
    "        \n",
    "        model_inputs = model.input\n",
    "        pretraining_output_seq_layer, pretraining_output_annoatations_layer = model.output\n",
    "        last_hidden_layer = pretraining_output_seq_layer if self.output_spec.output_type.is_seq else pretraining_output_annoatations_layer\n",
    "        last_hidden_layer = keras.layers.Dropout(self.dropout_rate)(last_hidden_layer)\n",
    "        \n",
    "        if self.output_spec.output_type.is_categorical:\n",
    "            output_layer = keras.layers.Dense(len(self.output_spec.unique_labels), activation = 'softmax')(last_hidden_layer)\n",
    "            loss = 'sparse_categorical_crossentropy'\n",
    "        elif self.output_spec.output_type.is_binary:\n",
    "            output_layer = keras.layers.Dense(1, activation = 'sigmoid')(last_hidden_layer)\n",
    "            loss = 'binary_crossentropy'\n",
    "        elif self.output_spec.output_type.is_numeric:\n",
    "            output_layer = keras.layers.Dense(1, activation = None)(last_hidden_layer)\n",
    "            loss = 'mse'\n",
    "        else:\n",
    "            raise ValueError('Unexpected global output type: %s' % self.output_spec.output_type)\n",
    "                \n",
    "        model = keras.models.Model(inputs = model_inputs, outputs = output_layer)\n",
    "        model.compile(loss = loss, optimizer =self.optimizer_class(learning_rate = self.lr, **self.other_optimizer_kwargs))\n",
    "        \n",
    "        self._init_weights(model)\n",
    "                \n",
    "        return model\n",
    "                        \n",
    "class InputEncoder:\n",
    "\n",
    "    def __init__(self, n_annotations):\n",
    "        self.n_annotations = n_annotations\n",
    "\n",
    "    def encode_X(self, seqs, seq_len):\n",
    "        return [\n",
    "            tokenize_seqs(seqs, seq_len),\n",
    "            np.zeros((len(seqs), self.n_annotations), dtype = np.int8)\n",
    "        ]\n",
    "        \n",
    "def load_pretrained_model_from_dump(dump_file_path, create_model_function, create_model_kwargs = {}, optimizer_class = keras.optimizers.Adam, lr = 2e-04, \\\n",
    "        other_optimizer_kwargs = {}, annots_loss_weight = 1, load_optimizer_weights = False):\n",
    "    \n",
    "    with open(dump_file_path, 'rb') as f:\n",
    "        n_annotations, model_weights, optimizer_weights = pickle.load(f)\n",
    "        \n",
    "    if not load_optimizer_weights:\n",
    "        optimizer_weights = None\n",
    "    \n",
    "    model_generator = PretrainingModelGenerator(create_model_function, n_annotations, create_model_kwargs = create_model_kwargs, optimizer_class = optimizer_class, lr = lr, \\\n",
    "            other_optimizer_kwargs = other_optimizer_kwargs, annots_loss_weight = annots_loss_weight, model_weights = model_weights, optimizer_weights = optimizer_weights)\n",
    "    input_encoder = InputEncoder(n_annotations)\n",
    "    \n",
    "    return model_generator, input_encoder\n",
    "\n",
    "def tokenize_seqs(seqs, seq_len):\n",
    "    # Note that tokenize_seq already adds <START> and <END> tokens.\n",
    "    return np.array([seq_tokens + (seq_len - len(seq_tokens)) * [additional_token_to_index['<PAD>']] for seq_tokens in map(tokenize_seq, seqs)], dtype = np.int32)\n",
    "    \n",
    "def clear_session():\n",
    "    import tensorflow.keras.backend as K\n",
    "    K.clear_session()\n",
    "    \n",
    "def copy_weights(weights):\n",
    "    return [_copy_number_or_array(w) for w in weights]\n",
    "    \n",
    "def _copy_number_or_array(variable):\n",
    "    if isinstance(variable, np.ndarray):\n",
    "        return variable.copy()\n",
    "    elif isinstance(variable, Number):\n",
    "        return variable\n",
    "    else:\n",
    "        raise TypeError('Unexpected type %s' % type(variable))\n",
    "    \n",
    "def _slice_arrays(arrays, slicing):\n",
    "    if isinstance(arrays, list) or isinstance(arrays, tuple):\n",
    "        return [array[slicing] for array in arrays]\n",
    "    else:\n",
    "        return arrays[slicing]\n",
    "\n",
    "ALL_AAS = 'ACDEFGHIKLMNPQRSTUVWXY'\n",
    "ADDITIONAL_TOKENS = ['<OTHER>', '<START>', '<END>', '<PAD>']\n",
    "\n",
    "# Each sequence is added <START> and <END> tokens\n",
    "ADDED_TOKENS_PER_SEQ = 2\n",
    "\n",
    "n_aas = len(ALL_AAS)\n",
    "aa_to_token_index = {aa: i for i, aa in enumerate(ALL_AAS)}\n",
    "additional_token_to_index = {token: i + n_aas for i, token in enumerate(ADDITIONAL_TOKENS)}\n",
    "token_to_index = {**aa_to_token_index, **additional_token_to_index}\n",
    "index_to_token = {index: token for token, index in token_to_index.items()}\n",
    "n_tokens = len(token_to_index)\n",
    "\n",
    "def tokenize_seq(seq):\n",
    "    other_token_index = additional_token_to_index['<OTHER>']\n",
    "    return [additional_token_to_index['<START>']] + [aa_to_token_index.get(aa, other_token_index) for aa in parse_seq(seq)] + \\\n",
    "            [additional_token_to_index['<END>']]\n",
    "            \n",
    "def parse_seq(seq):\n",
    "    if isinstance(seq, str):\n",
    "        return seq\n",
    "    elif isinstance(seq, bytes):\n",
    "        return seq.decode('utf8')\n",
    "    else:\n",
    "        raise TypeError('Unexpected sequence type: %s' % type(seq))\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "import importlib\n",
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "### Logging ###\n",
    "\n",
    "def log(*message, **kwargs):\n",
    "    \n",
    "    global _log_file\n",
    "    \n",
    "    end = kwargs.get('end', '\\n')\n",
    "    \n",
    "    if len(message) == 1:\n",
    "        message, = message\n",
    "    \n",
    "    full_message = '[%s] %s' % (format_now(), message)\n",
    "    \n",
    "    print(full_message, end = end)\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if log_file_open():\n",
    "        _log_file.write(full_message + end)\n",
    "        _log_file.flush()\n",
    "\n",
    "def start_log(log_dir, log_file_base_name):\n",
    "    \n",
    "    global _log_file\n",
    "    \n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "        \n",
    "    log_file_name = '%s__%d__%s.txt' % (log_file_base_name, os.getpid(), format_now())\n",
    "    \n",
    "    if not log_file_open():\n",
    "        print('Creating log file: %s' % log_file_name)\n",
    "        _log_file = open(os.path.join(log_dir, log_file_name), 'w')\n",
    "        \n",
    "def close_log():\n",
    "    \n",
    "    global _log_file\n",
    "    \n",
    "    if log_file_open():\n",
    "        _log_file.close()\n",
    "        del _log_file\n",
    "    \n",
    "def restart_log():\n",
    "    close_log()\n",
    "    start_log()\n",
    "    \n",
    "def log_file_open():\n",
    "    global _log_file\n",
    "    return '_log_file' in globals()\n",
    "    \n",
    "def create_time_measure_if_verbose(opening_statement, verbose):\n",
    "    if verbose:\n",
    "        return TimeMeasure(opening_statement)\n",
    "    else:\n",
    "        return DummyContext()\n",
    "\n",
    "\n",
    "### General ###\n",
    "\n",
    "def get_nullable(value, default_value):\n",
    "    if pd.isnull(value):\n",
    "        return default_value\n",
    "    else:\n",
    "        return value\n",
    "        \n",
    "        \n",
    "### Reflection ###\n",
    "\n",
    "def load_object(full_object_name):\n",
    "    name_parts = full_object_name.split('.')\n",
    "    object_name = name_parts[-1]\n",
    "    module_name = '.'.join(name_parts[:-1])\n",
    "    module = importlib.import_module(module_name)\n",
    "    return getattr(module, object_name)\n",
    "    \n",
    "    \n",
    "### Strings ###\n",
    "\n",
    "def trim(string, max_length, trim_suffix = '...'):\n",
    "    if len(string) <= max_length:\n",
    "        return string\n",
    "    else:\n",
    "        return string[:(max_length - len(trim_suffix))] + trim_suffix\n",
    "        \n",
    "def break_to_lines(text, max_line_len):\n",
    "    \n",
    "    lines = ['']\n",
    "    \n",
    "    for word in text.split():\n",
    "        \n",
    "        if len(lines[-1]) + len(word) > max_line_len:\n",
    "            lines.append('')\n",
    "            \n",
    "        if lines[-1] != '':\n",
    "            lines[-1] += ' '\n",
    "            \n",
    "        lines[-1] += word\n",
    "        \n",
    "    return '\\n'.join(lines)\n",
    "    \n",
    "    \n",
    "### IO ###\n",
    "\n",
    "def safe_symlink(src, dst, post_creation_hook = lambda created_symlink: None):\n",
    "    if os.path.exists(dst):\n",
    "        log('%s: already exists.' % dst)\n",
    "    else:\n",
    "        try:\n",
    "            os.symlink(src, dst)\n",
    "            post_creation_hook(dst)\n",
    "            log('Created link: %s -> %s' % (src, dst))\n",
    "        except OSError as e:\n",
    "            if e.errno == 17:\n",
    "                log('%s: already exists after all.' % dst)\n",
    "            else:\n",
    "                raise e\n",
    "        \n",
    "def safe_mkdir(path):\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as e:\n",
    "        assert 'File exists' in str(e), str(e)\n",
    "        \n",
    "def format_size_in_bytes(size):\n",
    "    \n",
    "    UNIT_RATIO = 1024\n",
    "    UNITS = ['B', 'KB', 'MB', 'GB', 'TB']\n",
    "    \n",
    "    for unit_index in range(len(UNITS)):\n",
    "        if size < UNIT_RATIO:\n",
    "            break\n",
    "        else:\n",
    "            size /= UNIT_RATIO\n",
    "            \n",
    "    return '%.1f%s' % (size, UNITS[unit_index])\n",
    "    \n",
    "def get_recognized_files_in_dir(dir_path, file_parser, log_unrecognized_files = True):\n",
    "    \n",
    "    recognized_files = []\n",
    "    unrecognized_files = []\n",
    "    \n",
    "    for file_name in os.listdir(dir_path):\n",
    "        try:\n",
    "            recognized_files.append((file_parser(file_name), file_name))\n",
    "        except:\n",
    "            if log_unrecognized_files:\n",
    "                unrecognized_files.append(file_name)\n",
    "                \n",
    "    if log_unrecognized_files and len(unrecognized_files) > 0:\n",
    "        log('%s: %d unrecognized files: %s' % (dir_path, len(unrecognized_files), ', '.join(unrecognized_files)))\n",
    "        \n",
    "    return list(sorted(recognized_files))\n",
    "    \n",
    "def monitor_memory(min_bytes_to_log = 1e08, max_elements_to_check = 100, collect_gc = True, del_output_variables = True, \\\n",
    "        list_like_types = [list, tuple, np.ndarray, pd.Series], dict_like_types = [dict, defaultdict]):\n",
    "    \n",
    "    already_monitored_object_ids = set()\n",
    "    \n",
    "    def _is_of_any_type(obj, types):\n",
    "        \n",
    "        for t in types:\n",
    "            if isinstance(obj, t):\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "        \n",
    "    def _check_len_limit(obj):\n",
    "        try:\n",
    "            return len(obj) <= max_elements_to_check\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def _log_object_if_needed(name, obj):\n",
    "\n",
    "        size = sys.getsizeof(obj)\n",
    "\n",
    "        if size >= min_bytes_to_log:\n",
    "            log('%s: %s' % (name, format_size_in_bytes(size)))\n",
    "            \n",
    "    def _monitor_object(name, obj):\n",
    "        if id(obj) not in already_monitored_object_ids:\n",
    "            \n",
    "            already_monitored_object_ids.add(id(obj))\n",
    "            _log_object_if_needed(name, obj)\n",
    "                        \n",
    "            if _is_of_any_type(obj, list_like_types) and _check_len_limit(obj):\n",
    "                for i, element in enumerate(obj):\n",
    "                    _monitor_object('%s[%d]' % (name, i), element)\n",
    "\n",
    "            if _is_of_any_type(obj, dict_like_types) and _check_len_limit(obj):\n",
    "                for key, value in obj.items():\n",
    "                    _monitor_object('%s[%s]' % (name, repr(key)), value)\n",
    "            \n",
    "            \n",
    "    for module_name, module in sys.modules.items():\n",
    "        for variable_name in dir(module):\n",
    "            \n",
    "            full_variable_name = variable_name if module_name == '__main__' else '%s.%s' % (module_name, variable_name)\n",
    "            _monitor_object(full_variable_name, getattr(module, variable_name))\n",
    "\n",
    "            if del_output_variables and module_name == '__main__' and re.match(r'^_[\\d_]+$', variable_name):\n",
    "                delattr(module, variable_name)\n",
    "                \n",
    "    if del_output_variables:\n",
    "        sys.modules['__main__'].Out = dict()\n",
    "        sys.modules['__main__']._oh = dict()\n",
    "\n",
    "    if collect_gc:\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "### Date & time ###\n",
    "\n",
    "def format_now():\n",
    "    return datetime.now().strftime('%Y_%m_%d-%H:%M:%S')\n",
    "    \n",
    "\n",
    "### Iterators & collections ###\n",
    "\n",
    "def compare_list_against_collection(input_list, collection):\n",
    "    collection_set = set(collection)\n",
    "    return [element for element in input_list if element in collection_set], [element for element in input_list if element not in collection_set]\n",
    "\n",
    "def get_chunk_slice(size, n_chunks, chunk_index):\n",
    "    assert size >= n_chunks\n",
    "    chunk_size = size / n_chunks\n",
    "    start_index = int(chunk_index * chunk_size)\n",
    "    end_index = int((chunk_index + 1) * chunk_size)\n",
    "    return start_index, end_index\n",
    "\n",
    "def get_chunk_intervals(size, chunk_size):\n",
    "    for start_index in range(0, size, chunk_size):\n",
    "        end_index = min(start_index + chunk_size, size)\n",
    "        yield start_index, end_index\n",
    "        \n",
    "def to_chunks(iterable, chunk_size):\n",
    "    \n",
    "    chunk = []\n",
    "    \n",
    "    for element in iterable:\n",
    "        \n",
    "        chunk.append(element)\n",
    "        \n",
    "        if len(chunk) >= chunk_size:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "            \n",
    "    if len(chunk) > 0:\n",
    "        yield chunk\n",
    "        \n",
    "def get_job_and_subjob_indices(n_jobs, n_tasks, task_index):\n",
    "    \n",
    "    '''\n",
    "    For example, if there are 170 tasks for working on 50 jobs, than each job will be divided to 3-4 tasks.\n",
    "    Since 170 % 50 = 20, the 20 first jobs will receive 4 tasks and the last 30 jobs will receive only 3 tasks.\n",
    "    In total, the first 80 tasks will be dedicated to jobs with 4 tasks each, and the 90 last tasks will be\n",
    "    dedicated to jobs with 3 tasks each. Hence, tasks 0-3 will go to job 0, tasks 4-7 will go to job 1, and so on;\n",
    "    tasks 80-82 will go to job 21, tasks 83-85 will job to job 22, and so on.  \n",
    "    '''\n",
    "    \n",
    "    assert n_tasks >= n_jobs\n",
    "    n_tasks_in_unprivileged_jobs = n_tasks // n_jobs\n",
    "    n_tasks_in_privileged_jobs = n_tasks_in_unprivileged_jobs + 1\n",
    "    n_privileged_jobs = n_tasks % n_jobs\n",
    "    n_tasks_of_privileged_jobs = n_tasks_in_privileged_jobs * n_privileged_jobs\n",
    "    \n",
    "    if task_index < n_tasks_of_privileged_jobs:\n",
    "        job_index = task_index // n_tasks_in_privileged_jobs\n",
    "        index_within_job = task_index % n_tasks_in_privileged_jobs\n",
    "        n_tasks_in_job = n_tasks_in_privileged_jobs\n",
    "    else:\n",
    "        task_index_in_unprivileged_group = task_index - n_tasks_of_privileged_jobs\n",
    "        job_index = n_privileged_jobs + task_index_in_unprivileged_group // n_tasks_in_unprivileged_jobs\n",
    "        index_within_job = task_index_in_unprivileged_group % n_tasks_in_unprivileged_jobs\n",
    "        n_tasks_in_job = n_tasks_in_unprivileged_jobs\n",
    "        \n",
    "    return job_index, index_within_job, n_tasks_in_job\n",
    "    \n",
    "def choose_from_cartesian_product(list_of_values, i, total = None):\n",
    "    \n",
    "    n = int(np.prod(list(map(len, list_of_values))))\n",
    "    \n",
    "    if total is not None:\n",
    "        assert n == total\n",
    "    \n",
    "    chosen_elements = []\n",
    "    \n",
    "    for values in list_of_values:\n",
    "        n //= len(values)\n",
    "        chosen_elements.append(values[i // n])\n",
    "        i %= n\n",
    "\n",
    "    return chosen_elements\n",
    "\n",
    "def calc_overlap_between_segments(ordered_segments1, ordered_segments2):\n",
    "    \n",
    "    '''\n",
    "    Calculates the total overlap size between a pair of ordered and disjoint groups of segments.\n",
    "    Each group of segment is given by: [(start1, end1), (start2, end2), ...]. \n",
    "    '''\n",
    "    \n",
    "    from interval_tree import IntervalTree\n",
    "    \n",
    "    if len(ordered_segments1) == 0 or len(ordered_segments2) == 0:\n",
    "        return 0\n",
    "    \n",
    "    if len(ordered_segments1) > len(ordered_segments2):\n",
    "        ordered_segments1, ordered_segments2 = ordered_segments2, ordered_segments1\n",
    "    \n",
    "    min_value = min(ordered_segments1[0][0], ordered_segments2[0][0])\n",
    "    max_value = max(ordered_segments1[-1][1], ordered_segments2[-1][1])\n",
    "    interval_tree1 = IntervalTree([segment + (segment,) for segment in ordered_segments1], min_value, max_value)\n",
    "    total_overlap = 0\n",
    "    \n",
    "    for segment in ordered_segments2:\n",
    "        for overlapping_segment in interval_tree1.find_range(segment):\n",
    "            overlapping_start = max(segment[0], overlapping_segment[0])\n",
    "            overlapping_end = min(segment[1], overlapping_segment[1])\n",
    "            assert overlapping_start <= overlapping_end, 'Reported overlap between %d..%d to %d..%d.' % (segment + \\\n",
    "                    overlapping_segment)\n",
    "            total_overlap += (overlapping_end - overlapping_start + 1)\n",
    "            \n",
    "    return total_overlap\n",
    "    \n",
    "def merge_lists_with_compatible_relative_order(lists):\n",
    "    \n",
    "    '''\n",
    "    Given a list of lists with compatible relative ordering (i.e. for every two sublists, the subset of elements that exist in the two\n",
    "    sublists will have the same relative order), returns a merging of these sublists into a single grand list that contains all the\n",
    "    elements (each element only once), and preserves the same ordering.\n",
    "    '''\n",
    "    \n",
    "    def merge_two_sublists(list1, list2):\n",
    "        \n",
    "        value_to_index = {value: float(i) for i, value in enumerate(list1)}\n",
    "        unique_list2_index = {}\n",
    "        last_identified_index = len(list1)\n",
    "        \n",
    "        for i, value in list(enumerate(list2))[::-1]:\n",
    "            if value in value_to_index:\n",
    "                last_identified_index = value_to_index[value]\n",
    "            else:\n",
    "                unique_list2_index[value] = last_identified_index - 1 + i / len(list2)\n",
    "                \n",
    "        value_to_index.update(unique_list2_index)\n",
    "        return sorted(value_to_index.keys(), key = value_to_index.get)\n",
    "    \n",
    "    return reduce(merge_two_sublists, lists, [])\n",
    "    \n",
    "    \n",
    "### argparse ###\n",
    "\n",
    "def get_parser_bool_type(parser):\n",
    "\n",
    "    def _bool_type(value):\n",
    "        if isinstance(value, bool):\n",
    "           return value\n",
    "        if value.lower() in ['yes', 'true', 't', 'y', '1']:\n",
    "            return True\n",
    "        elif value.lower() in ['no', 'false', 'f', 'n', '0']:\n",
    "            return False\n",
    "        else:\n",
    "            raise parser.error('\"%s\": unrecognized boolean value.' % value)\n",
    "            \n",
    "    return _bool_type\n",
    "\n",
    "def get_parser_file_type(parser, must_exist = False):\n",
    "\n",
    "    def _file_type(path):\n",
    "    \n",
    "        path = os.path.expanduser(path)\n",
    "    \n",
    "        if must_exist:\n",
    "            if not os.path.exists(path):\n",
    "                parser.error('File doesn\\'t exist: %s' % path)\n",
    "            elif not os.path.isfile(path):\n",
    "                parser.error('Not a file: %s' % path)\n",
    "            else:\n",
    "                return path\n",
    "        else:\n",
    "        \n",
    "            dir_path = os.path.dirname(path)\n",
    "        \n",
    "            if dir_path and not os.path.exists(dir_path):\n",
    "                parser.error('Parent directory doesn\\'t exist: %s' % dir_path)\n",
    "            else:\n",
    "                return path\n",
    "    \n",
    "    return _file_type\n",
    "\n",
    "def get_parser_directory_type(parser, create_if_not_exists = False):\n",
    "    \n",
    "    def _directory_type(path):\n",
    "    \n",
    "        path = os.path.expanduser(path)\n",
    "    \n",
    "        if not os.path.exists(path):\n",
    "            if create_if_not_exists:\n",
    "            \n",
    "                parent_path = os.path.dirname(path)\n",
    "            \n",
    "                if parent_path and not os.path.exists(parent_path):\n",
    "                    parser.error('Cannot create empty directory (parent directory doesn\\'t exist): %s' % path)\n",
    "                else:\n",
    "                    os.mkdir(path)\n",
    "                    return path\n",
    "            else:\n",
    "                parser.error('Path doesn\\'t exist: %s' % path)\n",
    "        elif not os.path.isdir(path):\n",
    "            parser.error('Not a directory: %s' % path)\n",
    "        else:\n",
    "            return path\n",
    "        \n",
    "    return _directory_type\n",
    "    \n",
    "def add_parser_task_arguments(parser):\n",
    "    parser.add_argument('--task-index', dest = 'task_index', metavar = '<0,...,N_TASKS-1>', type = int, default = None, help = 'If you want to ' + \\\n",
    "            ' distribute this process across multiple computation resources (e.g. on a cluster) you can specify the total number of tasks ' + \\\n",
    "            '(--total-tasks) to split it into, and the index of the current task to run (--task-index).')\n",
    "    parser.add_argument('--total-tasks', dest = 'total_tasks', metavar = '<N_TASKS>', type = int, default = None, help = 'See --task-index.')\n",
    "    parser.add_argument('--task-index-env-variable', dest = 'task_index_env_variable', metavar = '<e.g. SLURM_ARRAY_TASK_ID>', type = str, default = None, \\\n",
    "            help = 'Instead of specifying a hardcoded --task-index, you can specify an environtment variable to take it from (e.g. SLURM_ARRAY_TASK_ID ' + \\\n",
    "            'if you use SLURM to distribute the jobs).')\n",
    "    parser.add_argument('--total-tasks-env-variable', dest = 'total_tasks_env_variable', metavar = '<e.g. SLURM_ARRAY_TASK_COUNT>', type = str, \\\n",
    "            default = None, help = 'Instead of specifying a hardcoded --total-tasks, you can specify an environtment variable to take it from (e.g. ' + \\\n",
    "            'SLURM_ARRAY_TASK_COUNT if you use SLURM to distribute the jobs).')\n",
    "            \n",
    "def determine_parser_task_details(args):\n",
    "    \n",
    "    if args.task_index is not None and args.task_index_env_variable is not None:\n",
    "        parser.error('You must choose between --task-index and --task-index-env-variable.')\n",
    "    \n",
    "    if args.task_index is not None:\n",
    "        task_index = args.task_index\n",
    "    elif args.task_index_env_variable is not None:\n",
    "        task_index = int(os.getenv(args.task_index_env_variable))\n",
    "    else:\n",
    "        task_index = None\n",
    "        \n",
    "    if args.total_tasks is not None and args.total_tasks_env_variable is not None:\n",
    "        parser.error('You must choose between --total-tasks and --total-tasks-env-variable.')\n",
    "        \n",
    "    if args.total_tasks is not None:\n",
    "        total_tasks = args.total_tasks\n",
    "    elif args.total_tasks_env_variable is not None:\n",
    "        total_tasks = int(os.getenv(args.total_tasks_env_variable))\n",
    "    else:\n",
    "        total_tasks = None\n",
    "\n",
    "    if task_index is None and total_tasks is None:\n",
    "        task_index = 0\n",
    "        total_tasks = 1\n",
    "    elif task_index is None or total_tasks is None:\n",
    "        parser.error('Task index and total tasks must either be specified or unspecified together.')\n",
    "    \n",
    "    if task_index < 0 or task_index >= total_tasks:\n",
    "        parser.error('Task index must be in the range 0,...,(total tasks)-1.')\n",
    "    \n",
    "    return task_index, total_tasks\n",
    "\n",
    "    \n",
    "### Numpy ###\n",
    "\n",
    "def normalize(x):\n",
    "\n",
    "    if isinstance(x, list):\n",
    "        x = np.array(x)\n",
    "\n",
    "    u = np.mean(x)\n",
    "    sigma = np.std(x)\n",
    "    \n",
    "    if sigma == 0:\n",
    "        return np.ones_like(x)\n",
    "    else:\n",
    "        return (x - u) / sigma\n",
    "    \n",
    "def random_mask(size, n_trues):\n",
    "    assert n_trues <= size\n",
    "    mask = np.full(size, False)\n",
    "    mask[:n_trues] = True\n",
    "    np.random.shuffle(mask)\n",
    "    return mask\n",
    "    \n",
    "def indices_to_masks(n, indices):\n",
    "    positive_mask = np.zeros(n, dtype = bool)\n",
    "    positive_mask[indices] = True\n",
    "    negative_mask = np.ones(n, dtype = bool)\n",
    "    negative_mask[indices] = False\n",
    "    return positive_mask, negative_mask\n",
    "    \n",
    "def as_hot_encoding(values, value_to_index, n_values = None):\n",
    "\n",
    "    if n_values is None:\n",
    "        n_values = len(value_to_index)\n",
    "        \n",
    "    result = np.zeros(n_values)\n",
    "    \n",
    "    try:\n",
    "        values = iter(values)\n",
    "    except TypeError:\n",
    "        values = iter([values])\n",
    "        \n",
    "    for value in values:\n",
    "        result[value_to_index[value]] += 1\n",
    "        \n",
    "def is_full_rank(matrix):\n",
    "    return np.linalg.matrix_rank(matrix) == min(matrix.shape)\n",
    "    \n",
    "def find_linearly_independent_columns(matrix):\n",
    "    \n",
    "    '''\n",
    "    The calculation is fasciliated by the Gram Schmidt process, everytime taking the next column and removing its projections\n",
    "    from all next columns, getting rid of columns which end up zero.\n",
    "    '''\n",
    "    \n",
    "    n_rows, n_cols = matrix.shape\n",
    "    \n",
    "    if np.linalg.matrix_rank(matrix) == n_cols:\n",
    "        return np.arange(n_cols)\n",
    "    \n",
    "    orthogonalized_matrix = matrix.copy().astype(float)\n",
    "    independent_columns = []\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        if not np.isclose(orthogonalized_matrix[:, i], 0).all():\n",
    "            \n",
    "            independent_columns.append(i)\n",
    "            \n",
    "            if len(independent_columns) >= n_rows:\n",
    "                break\n",
    "            \n",
    "            orthogonalized_matrix[:, i] = orthogonalized_matrix[:, i] / np.linalg.norm(orthogonalized_matrix[:, i])\n",
    "            \n",
    "            if i < n_cols - 1:\n",
    "                # Remove the projection of the ith column from all next columns\n",
    "                orthogonalized_matrix[:, (i + 1):] -= np.dot(orthogonalized_matrix[:, i], \\\n",
    "                        orthogonalized_matrix[:, (i + 1):]).reshape(1, -1) * orthogonalized_matrix[:, i].reshape(-1, 1)\n",
    "            \n",
    "    return np.array(independent_columns)\n",
    "\n",
    "def transpose_dataset(src, dst, max_memory_bytes, flush_func = None):\n",
    "    \n",
    "    n_rows, n_cols = src.shape[:2]\n",
    "    entry_nbytes = src[:1, :1].nbytes\n",
    "    ideal_entries_per_chunk = max_memory_bytes / entry_nbytes\n",
    "    ideal_chunk_size = np.sqrt(ideal_entries_per_chunk)\n",
    "    \n",
    "    if n_rows <= n_cols:\n",
    "        row_chunk_size = min(int(ideal_chunk_size), n_rows)\n",
    "        col_chunk_size = min(int(ideal_entries_per_chunk / row_chunk_size), n_cols)\n",
    "    else:\n",
    "        col_chunk_size = min(int(ideal_chunk_size), n_cols)\n",
    "        row_chunk_size = min(int(ideal_entries_per_chunk / col_chunk_size), n_rows)\n",
    "        \n",
    "    log('Will use chunks of size %dx%d to transpose a %dx%d matrix.' % (row_chunk_size, col_chunk_size, n_rows, n_cols))\n",
    "    \n",
    "    for row_start, row_end in get_chunk_intervals(n_rows, row_chunk_size):\n",
    "        for col_start, col_end in get_chunk_intervals(n_cols, col_chunk_size):\n",
    "            \n",
    "            log('Transposing chunk (%d..%d)x(%d..%d)...' % (row_start, row_end - 1, col_start, col_end - 1))\n",
    "            dst[col_start:col_end, row_start:row_end] = src[row_start:row_end, col_start:col_end].transpose()\n",
    "            \n",
    "            if flush_func is not None:\n",
    "                flush_func()\n",
    "                \n",
    "    log('Finished transposing.')\n",
    "\n",
    "\n",
    "### Pandas ###\n",
    "\n",
    "def summarize(df, n = 5, sample = False):\n",
    "    \n",
    "    from IPython.display import display\n",
    "    \n",
    "    if sample:\n",
    "        display(df.sample(n))\n",
    "    else:\n",
    "        display(df.head(n))\n",
    "    \n",
    "    print('%d records' % len(df))\n",
    "    \n",
    "def nullable_idxmin(series):\n",
    "    \n",
    "    result = series.idxmin()\n",
    "    \n",
    "    if pd.isnull(result):\n",
    "        if len(series) == 0:\n",
    "            return np.nan\n",
    "        else:\n",
    "            return series.index[0]\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "def get_first_value(df):\n",
    "    '''\n",
    "    Will return a Series with the same index. For each row the value will be that of the first column which is not null.\n",
    "    '''\n",
    "    col_idxs = np.argmax(pd.notnull(df).values, axis = 1)\n",
    "    return pd.Series(df.values[np.arange(len(df)), col_idxs], index = df.index)\n",
    "    \n",
    "def slice_not_in_index(df_or_series, index_to_exclude):\n",
    "    mask = pd.Series(True, index = df_or_series.index)\n",
    "    mask.loc[index_to_exclude] = False\n",
    "    return df_or_series.loc[mask]\n",
    "    \n",
    "def swap_series_index_and_value(series):\n",
    "    return pd.Series(series.index, index = series.values)\n",
    "    \n",
    "def concat_dfs_with_partial_columns(dfs):\n",
    "    columns = max([df.columns for df in dfs], key = len)\n",
    "    assert all([set(df.columns) <= set(columns) for df in dfs])\n",
    "    return pd.concat(dfs, sort = False)[columns]\n",
    "    \n",
    "def concat_dfs_with_compatible_columns(dfs):\n",
    "    columns = merge_lists_with_compatible_relative_order([df.columns for df in dfs])\n",
    "    return pd.concat(dfs, sort = False)[columns]\n",
    "\n",
    "def safe_get_df_group(df_groupby, group_name):\n",
    "    if group_name in df_groupby.groups:\n",
    "        return df_groupby.get_group(group_name)\n",
    "    else:\n",
    "        _, some_group_df = next(iter(df_groupby))\n",
    "        return pd.DataFrame(columns = some_group_df.columns)\n",
    "        \n",
    "def bin_groupby(df, series_or_col_name, n_bins):\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        return df\n",
    "    \n",
    "    if isinstance(series_or_col_name, str):\n",
    "        series = df[series_or_col_name]\n",
    "    else:\n",
    "        series = series_or_col_name\n",
    "        \n",
    "    min_value, max_value = series.min(), series.max()\n",
    "    bin_size = (max_value - min_value) / n_bins\n",
    "    \n",
    "    bind_ids = ((series - min_value) / bin_size).astype(int)\n",
    "    bind_ids[bind_ids >= n_bins] = n_bins - 1\n",
    "    \n",
    "    return df.groupby(bind_ids)\n",
    "    \n",
    "def value_df_to_hot_encoding_df(value_df, value_headers = {}):\n",
    "    \n",
    "    flat_values = value_df.values.flatten()\n",
    "    all_values = sorted(np.unique(flat_values[pd.notnull(flat_values)]))\n",
    "    value_to_index = {value: i for i, value in enumerate(all_values)}\n",
    "    hot_encoding_matrix = np.zeros((len(value_df), len(all_values)))\n",
    "    \n",
    "    for _, column_values in value_df.iteritems():\n",
    "        row_position_to_value_index = column_values.reset_index(drop = True).dropna().map(value_to_index)\n",
    "        hot_encoding_matrix[row_position_to_value_index.index.values, row_position_to_value_index.values] = 1\n",
    "    \n",
    "    headers = [value_headers.get(value, value) for value in all_values]\n",
    "    return pd.DataFrame(hot_encoding_matrix, index = value_df.index, columns = headers)\n",
    "    \n",
    "def set_series_to_hot_encoding_df(set_series, value_headers = {}):\n",
    "\n",
    "    all_values = sorted(set.union(*set_series))\n",
    "    value_to_index = {value: i for i, value in enumerate(all_values)}\n",
    "    hot_encoding_matrix = np.zeros((len(set_series), len(all_values)))\n",
    "    \n",
    "    for i, record_values in enumerate(set_series):\n",
    "        hot_encoding_matrix[i, [value_to_index[value] for value in record_values]] = 1\n",
    "        \n",
    "    headers = [value_headers.get(value, value) for value in all_values]\n",
    "    return pd.DataFrame(hot_encoding_matrix, index = set_series.index, columns = headers)\n",
    "    \n",
    "def resolve_dummy_variable_trap(hot_encoding_df, validate_completeness = True, inplace = False, verbose = True):\n",
    "\n",
    "    '''\n",
    "    When using one-hot-encoding in regression, there is a problem of encoding all possible variables if also using an intercept/const variable,\n",
    "    because then the variables end up linearly dependent (a singular matrix is problematic with many implementations of regression). See for\n",
    "    example: https://www.algosome.com/articles/dummy-variable-trap-regression.html\n",
    "    To resolve this issue, this function will remove the most frequent column (to minimize the chance of any subset of the rows resulting a\n",
    "    matrix which is not fully ranked).\n",
    "    '''\n",
    "    \n",
    "    # Validate we are indeed dealing with one-hot-encoding.\n",
    "    assert set(np.unique(hot_encoding_df.values).astype(float)) <= {0.0, 1.0}\n",
    "    \n",
    "    if validate_completeness:\n",
    "        assert (hot_encoding_df.sum(axis = 1) == 1).all()\n",
    "    else:\n",
    "        assert (hot_encoding_df.sum(axis = 1) <= 1).all()\n",
    "    \n",
    "    most_frequent_variable = hot_encoding_df.sum().idxmax()\n",
    "    \n",
    "    if verbose:\n",
    "        log('To avoid the \"dummy variable trap\", removing the %s column (%d matching records).' % (most_frequent_variable, \\\n",
    "                hot_encoding_df[most_frequent_variable].sum()))\n",
    "    \n",
    "    if inplace:\n",
    "        del hot_encoding_df[most_frequent_variable]\n",
    "    else:\n",
    "        return hot_encoding_df[[column_name for column_name in hot_encoding_df.columns if column_name != most_frequent_variable]]\n",
    "    \n",
    "def set_constant_row(df, row_mask, row_values):\n",
    "    df[row_mask] = np.tile(row_values, (row_mask.sum(), 1))\n",
    "    \n",
    "def construct_df_from_rows(row_repertoire, row_indexer):\n",
    "    \n",
    "    result = pd.DataFrame(index = row_indexer.index, columns = row_repertoire.columns)\n",
    "    \n",
    "    for row_index, row_values in row_repertoire.iterrows():\n",
    "        set_constant_row(result, row_indexer == row_index, row_values)\n",
    "        \n",
    "    return result\n",
    "    \n",
    "def get_row_last_values(df):\n",
    "    \n",
    "    result = pd.Series(np.nan, index = df.index)\n",
    "\n",
    "    for column in df.columns[::-1]:\n",
    "        result = result.where(pd.notnull(result), df[column])\n",
    "\n",
    "    return result\n",
    "    \n",
    "def are_close_dfs(df1, df2, rtol = 1e-05, atol = 1e-08):\n",
    "    \n",
    "    assert (df1.dtypes == df2.dtypes).all()\n",
    "    \n",
    "    for column, dtype in df1.dtypes.iteritems():\n",
    "        \n",
    "        if np.issubdtype(dtype, np.float):\n",
    "            cmp_series = np.isclose(df1[column], df2[column], rtol = rtol, atol = atol) | (pd.isnull(df1[column]) & \\\n",
    "                    pd.isnull(df2[column]))\n",
    "        else:\n",
    "            cmp_series = (df1[column] == df2[column])\n",
    "            \n",
    "        if not cmp_series.all():\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "    \n",
    "def append_df_to_excel(excel_writer, df, sheet_name, index = True):\n",
    "    \n",
    "    header_format = excel_writer.book.add_format({'bold': True})\n",
    "     \n",
    "    df.to_excel(excel_writer, sheet_name, index = index)\n",
    "    worksheet = excel_writer.sheets[sheet_name]\n",
    "    \n",
    "    for column_index, column_name in enumerate(df.columns):\n",
    "        worksheet.write(0, column_index + int(index), column_name, header_format)\n",
    "        \n",
    "    if index:\n",
    "        for row_index_number, row_index_value in enumerate(df.index):\n",
    "            worksheet.write(row_index_number + 1, 0, row_index_value)\n",
    "        \n",
    "def is_binary_series(series):\n",
    "\n",
    "    # First validating that the type of the series is convertable to float.\n",
    "    try:\n",
    "        float(series.iloc[0])\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "    return set(series.unique().astype(float)) <= {0.0, 1.0}\n",
    "    \n",
    "def resolve_quasi_complete_separation_by_removing_binary_columns(X, y):\n",
    "    \n",
    "    '''\n",
    "    When performing logistic regression of y against X, the matrix X must be of full rank; otherwise (i.e. if the columns of X are\n",
    "    linearly dependent), then statsmodel's Logit model gives a singular-matrix error. It also appears that quasi-complete separation\n",
    "    causes troubles, namely if the columns of X are linearly dependent conditioned on y. In other words, assuming that y is binary,\n",
    "    we need that X[y, :] would still be of full rank (we assume that the vast majority of records have a negative y value, and only\n",
    "    a small fraction have a positive value, so given that X is of full rank we need not worry about X[~y, :]). To resolve this problem,\n",
    "    this function will remove binary columns of X until X[y, :] is of full rank. Whenever a column of X is removed, we also remove the\n",
    "    corresponding records (rows of X and y) that have those values (so if a removed column represent some covariate, e.g. a certain\n",
    "    batch, we also remove all the samples from this batch in order for not having any covariates not accounted for).\n",
    "    @param X (pd.DataFrame): The exogenous variables (rows are records, columns are variables).\n",
    "    @pram y (pd.Series): The endogenous variable (must have the same index as X).\n",
    "    '''\n",
    "    \n",
    "    row_mask = pd.Series(True, index = X.index)\n",
    "    \n",
    "    if not is_binary_series(y):\n",
    "        return X, y, X.columns, set(), row_mask\n",
    "        \n",
    "    boolean_y = y.astype(bool)\n",
    "    all_kept_binary_columns = np.array([column_name for column_name in X.columns if is_binary_series(X[column_name])])\n",
    "    # We sort the binary columns by how common they are, so when we start removing them, we will give priority to the more common ones\n",
    "    # (i.e. remove the least frequent first).\n",
    "    all_kept_binary_columns = X[all_kept_binary_columns].sum().sort_values(ascending = False).index\n",
    "    all_removed_binary_columns = set()\n",
    "    \n",
    "    while len(all_kept_binary_columns) > 0:\n",
    "    \n",
    "        positive_X = X.loc[row_mask & boolean_y, all_kept_binary_columns]\n",
    "        old_all_kept_binary_columns = all_kept_binary_columns\n",
    "        all_kept_binary_columns = all_kept_binary_columns[find_linearly_independent_columns(positive_X.values)]\n",
    "        columns_to_remove = set(old_all_kept_binary_columns) - set(all_kept_binary_columns)\n",
    "        \n",
    "        for column_name in columns_to_remove:\n",
    "            log('Removing the columns %s (%d occurances) to avoid quasi-complete separation.' % (column_name, X[column_name].sum()))\n",
    "            all_removed_binary_columns.add(column_name)\n",
    "            row_mask &= (~X[column_name].astype(bool))\n",
    "            \n",
    "        if len(columns_to_remove) == 0:\n",
    "            break\n",
    "\n",
    "    if not row_mask.all():\n",
    "        log('Overall removed %d columns occuring in %d records to avoid quasi-complete separation.' % (len(all_removed_binary_columns), \\\n",
    "                (~row_mask).sum()))\n",
    "        \n",
    "    retained_columns = [column_name for column_name in X.columns if column_name not in all_removed_binary_columns]\n",
    "    X = X.loc[row_mask, retained_columns]\n",
    "    y = y.loc[row_mask]\n",
    "    \n",
    "    return X, y, retained_columns, all_removed_binary_columns, row_mask\n",
    "\n",
    "    \n",
    "### Statistics ###\n",
    "\n",
    "def to_normal_z_values(raw_values):\n",
    "\n",
    "    from scipy.stats import rankdata, norm\n",
    "    \n",
    "    pvals = (rankdata(raw_values) - 0.5) / len(raw_values)\n",
    "    normal_z_values = norm.ppf(pvals)\n",
    "    \n",
    "    if isinstance(raw_values, pd.Series):\n",
    "        return pd.Series(normal_z_values, index = raw_values.index)\n",
    "    else:\n",
    "        return normal_z_values\n",
    "\n",
    "def multipletests_with_nulls(values, method = 'fdr_bh'):\n",
    "\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "    \n",
    "    significance = np.zeros(len(values), dtype = bool)\n",
    "    qvals = np.nan * np.empty(len(values))\n",
    "    mask = pd.notnull(values)\n",
    "    \n",
    "    if mask.any():\n",
    "        significance[np.array(mask)], qvals[np.array(mask)], _, _ = multipletests(values[mask], method = method)\n",
    "    \n",
    "    return significance, qvals\n",
    "    \n",
    "def test_enrichment(mask1, mask2):\n",
    "\n",
    "    from scipy.stats import fisher_exact\n",
    "\n",
    "    assert len(mask1) == len(mask2)\n",
    "    \n",
    "    n1 = mask1.sum()\n",
    "    n2 = mask2.sum()\n",
    "    n_both = (mask1 & mask2).sum()\n",
    "    n_total = len(mask1)\n",
    "    n_expected = n1 * n2 / n_total\n",
    "    enrichment_factor = n_both / n_expected\n",
    "    \n",
    "    contingency_table = np.array([\n",
    "        [(mask1 & mask2).sum(), (mask1 & (~mask2)).sum()],\n",
    "        [((~mask1) & mask2).sum(), ((~mask1) & (~mask2)).sum()],\n",
    "    ])\n",
    "    _, pval = fisher_exact(contingency_table)\n",
    "    \n",
    "    return n1, n2, n_both, n_total, n_expected, enrichment_factor, contingency_table, pval\n",
    "    \n",
    "def test_enrichment_sets(set1, set2, n_total):\n",
    "\n",
    "    from scipy.stats import fisher_exact\n",
    "    \n",
    "    n1 = len(set1)\n",
    "    n2 = len(set2)\n",
    "    n_both = len(set1 & set2)\n",
    "    n_expected = n1 * n2 / n_total\n",
    "    enrichment_factor = n_both / n_expected\n",
    "    \n",
    "    contingency_table = np.array([\n",
    "        [n_both, n1 - n_both],\n",
    "        [n2 - n_both, n_total - n1 - n2 + n_both],\n",
    "    ])\n",
    "    _, pval = fisher_exact(contingency_table)\n",
    "    \n",
    "    return n1, n2, n_both, n_total, n_expected, enrichment_factor, contingency_table, pval\n",
    "    \n",
    "    \n",
    "### h5f ###\n",
    "    \n",
    "def flush_h5_file(h5f):\n",
    "    h5f.flush()\n",
    "    os.fsync(h5f.id.get_vfd_handle())\n",
    "    \n",
    "def transpose_h5f_dataset(h5f, src_name, dst_name, max_memory_bytes):\n",
    "    flush_func = lambda: flush_h5_file(h5f)\n",
    "    src = h5f[src_name]\n",
    "    nrows, ncols = src.shape[:2]\n",
    "    dst = h5f.create_dataset(dst_name, shape = (ncols, nrows), dtype = src.dtype)\n",
    "    transpose_dataset(src, dst, max_memory_bytes, flush_func)\n",
    "    \n",
    "    \n",
    "### Matplotlib ###\n",
    "\n",
    "def draw_rectangle(ax, start_x, end_x, start_y, end_y, **kwargs):\n",
    "    from matplotlib import patches\n",
    "    ax.add_patch(patches.Rectangle((start_x, start_y), end_x - start_x, end_y - start_y, **kwargs))\n",
    "    \n",
    "def set_ax_border_color(ax, color):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for child in ax.get_children():\n",
    "        if isinstance(child, plt.matplotlib.spines.Spine):\n",
    "            child.set_color(color)\n",
    "    \n",
    "def plot_prediction_scatter(y_pred, y_true, value = 'value'):\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    log(pearsonr(y_pred, y_true))\n",
    "    log(spearmanr(y_pred, y_true))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (10, 6))\n",
    "    ax.scatter(y_pred, y_true)\n",
    "    ax.set_xlabel('Predicted %s' % value)\n",
    "    ax.set_ylabel('Actual %s' % value)\n",
    "    \n",
    "def draw_pvals_qq_plot(pvals, max_density = 100, min_pval = None, ax = None, figsize = (7, 7), scatter_options = {}, \\\n",
    "        xlabel = 'Expected p-values (-log10)', ylabel = 'Observed p-values (-log10)'):\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    if 'color' not in scatter_options:\n",
    "        scatter_options['color'] = '#2e75b6'\n",
    "    \n",
    "    pvals = np.array(pvals)\n",
    "    \n",
    "    if min_pval is not None:\n",
    "        pvals = np.maximum(pvals, min_pval)\n",
    "    \n",
    "    n_total_pvals = len(pvals)\n",
    "    sorted_mlog_pvals = np.sort(-np.log10(pvals))\n",
    "    max_mlog_pval = sorted_mlog_pvals.max()\n",
    "    \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize = figsize)\n",
    "    \n",
    "    ax.plot([0, max_mlog_pval], [0, max_mlog_pval], color = 'red', linestyle = '--', alpha = 0.5)\n",
    "    ax.set_xlim((0, max_mlog_pval))\n",
    "    ax.set_ylim((0, max_mlog_pval))\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    \n",
    "    for upper_limit in range(1, int(max_mlog_pval + 3)):\n",
    "        \n",
    "        n_remained_pvals = len(sorted_mlog_pvals)\n",
    "        i = np.searchsorted(sorted_mlog_pvals, upper_limit)\n",
    "        range_pvals = sorted_mlog_pvals[:i]\n",
    "        sorted_mlog_pvals = sorted_mlog_pvals[i:]\n",
    "        \n",
    "        if len(range_pvals) > 0:\n",
    "                    \n",
    "            if len(range_pvals) <= max_density:\n",
    "                range_chosen_indices = np.arange(len(range_pvals))\n",
    "            else:\n",
    "                # We want to choose the p-values uniformly in the space of their expected frequencies (i.e. sampling more towards the higher end of the\n",
    "                # spectrum).\n",
    "                range_min_mlog_freq = -np.log10(n_remained_pvals / n_total_pvals)\n",
    "                range_max_mlog_freq = -np.log10((n_remained_pvals - len(range_pvals) + 1) / n_total_pvals)\n",
    "                range_chosen_mlog_freqs = np.linspace(range_min_mlog_freq, range_max_mlog_freq, max_density)\n",
    "                range_chosen_freqs = np.power(10, -range_chosen_mlog_freqs)\n",
    "                # Once having the desired freqs, reverse the function to get the indices that provide them\n",
    "                range_chosen_indices = np.unique((n_remained_pvals - n_total_pvals * range_chosen_freqs).astype(int))\n",
    "\n",
    "            range_pvals = range_pvals[range_chosen_indices]\n",
    "            range_freqs = (n_remained_pvals - range_chosen_indices) / n_total_pvals\n",
    "            range_mlog_freqs = -np.log10(range_freqs)\n",
    "            ax.scatter(range_mlog_freqs, range_pvals, **scatter_options)\n",
    "            \n",
    "def draw_manhattan_plot(gwas_results, significance_treshold = 5e-08, max_results_to_plot = 1e06, \\\n",
    "        pval_threshold_to_force_inclusion = 1e-03, min_pval = 1e-300, ax = None, figsize = (12, 6), \\\n",
    "        s = 1.5, chrom_to_color = None):\n",
    "    \n",
    "    '''\n",
    "    gwas_results (pd.DataFrame): Should have the following columns:\n",
    "    - chromosome (str)\n",
    "    - position (int)\n",
    "    - pval (float)\n",
    "    '''\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "        \n",
    "    CHROMS = list(map(str, range(1, 23))) + ['X', 'Y']\n",
    "    CHROM_TO_COLOR = {'1': '#0100fb', '2': '#ffff00', '3': '#00ff03', '4': '#bfbfbf', '5': '#acdae9', '6': '#a020f1',\n",
    "            '7': '#ffa502', '8': '#ff00fe', '9': '#fe0000', '10': '#90ee90', '11': '#a52929', '12': '#000000', \n",
    "            '13': '#ffbfcf', '14': '#4484b2', '15': '#b63063', '16': '#f8816f', '17': '#ed84f3', '18': '#006401',\n",
    "            '19': '#020184', '20': '#ced000', '21': '#cd0001', '22': '#050098', 'X': '#505050', 'Y': '#ff8000'}\n",
    "    \n",
    "    if chrom_to_color is None:\n",
    "        chrom_to_color = CHROM_TO_COLOR\n",
    "    \n",
    "    if len(gwas_results) > max_results_to_plot:\n",
    "        mask = pd.Series(random_mask(len(gwas_results), int(max_results_to_plot)), index = gwas_results.index)\n",
    "        mask[gwas_results['pval'] <= pval_threshold_to_force_inclusion] = True\n",
    "        gwas_results = gwas_results[mask]\n",
    "    \n",
    "    max_pos_per_chrom = gwas_results.groupby('chromosome')['position'].max()\n",
    "    accumulating_pos = 0\n",
    "    chrom_accumulating_positions = []\n",
    "    \n",
    "    for chrom in CHROMS:\n",
    "        if chrom in max_pos_per_chrom.index:\n",
    "            chrom_accumulating_positions.append((chrom, accumulating_pos + 1, accumulating_pos + max_pos_per_chrom[chrom]))\n",
    "            accumulating_pos += max_pos_per_chrom[chrom]\n",
    "            \n",
    "    chrom_accumulating_positions = pd.DataFrame(chrom_accumulating_positions, columns = ['chromosome', \\\n",
    "            'accumulating_start_position', 'accumulating_end_position']).set_index('chromosome', drop = True)\n",
    "    chrom_middle_accumulating_positions = (chrom_accumulating_positions['accumulating_start_position'] + \\\n",
    "            chrom_accumulating_positions['accumulating_end_position']) / 2\n",
    "        \n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(figsize = figsize)\n",
    "    \n",
    "    ax.set_facecolor('white')\n",
    "    plt.setp(ax.spines.values(), color = '#444444')\n",
    "    ax.grid(False)\n",
    "    \n",
    "    if significance_treshold is not None:\n",
    "        ax.axhline(y = -np.log10(significance_treshold), linestyle = '--', linewidth = 1, color = 'red')\n",
    "    \n",
    "    gwas_results_per_chrom = gwas_results.groupby('chromosome')\n",
    "    max_y = 0\n",
    "    \n",
    "    for chrom in chrom_accumulating_positions.index:\n",
    "        chrom_gwas_results = gwas_results_per_chrom.get_group(chrom)\n",
    "        chrom_gwas_accumulating_positions = chrom_accumulating_positions.loc[chrom, 'accumulating_start_position'] + \\\n",
    "                chrom_gwas_results['position']\n",
    "        chrom_gwas_minus_log_pval = -np.log10(np.maximum(chrom_gwas_results['pval'], min_pval))\n",
    "        max_y = max(max_y, chrom_gwas_minus_log_pval.max())\n",
    "        ax.scatter(chrom_gwas_accumulating_positions, chrom_gwas_minus_log_pval, color = chrom_to_color[chrom], s = s)\n",
    "        \n",
    "    ax.set_xlabel('Chromosome')\n",
    "    ax.set_ylabel('-log10(p-value)')\n",
    "    ax.set_xticks(chrom_middle_accumulating_positions)\n",
    "    ax.set_xticklabels(chrom_middle_accumulating_positions.index)\n",
    "    ax.set_xlim(1, accumulating_pos)\n",
    "    ax.set_ylim(0, max_y + 1)\n",
    "    \n",
    "    return ax\n",
    "    \n",
    "    \n",
    "### Biopython Helper Functions ###\n",
    "\n",
    "def as_biopython_seq(seq):\n",
    "\n",
    "    from Bio.Seq import Seq\n",
    "\n",
    "    if isinstance(seq, Seq):\n",
    "        return seq\n",
    "    elif isinstance(seq, str):\n",
    "        return Seq(seq)\n",
    "    else:\n",
    "        raise Exception('Cannot resolve type %s as Biopython Seq' % type(seq))\n",
    "            \n",
    "            \n",
    "### Slurm ###\n",
    "\n",
    "def get_slurm_job_array_ids(parse_total_tasks_by_max_variable = True, log_ids = True, verbose = True, task_index_remapping_json_file_path = None):\n",
    "\n",
    "    job_id = int(os.getenv('SLURM_ARRAY_JOB_ID'))\n",
    "    task_index = int(os.getenv('SLURM_ARRAY_TASK_ID'))\n",
    "    \n",
    "    if 'TASK_ID_OFFSET' in os.environ:\n",
    "        \n",
    "        task_offset = int(os.getenv('TASK_ID_OFFSET'))\n",
    "        \n",
    "        if verbose:\n",
    "            log('Raw task index %d with offset %d.' % (task_index, task_offset))\n",
    "        \n",
    "        task_index += task_offset\n",
    "        \n",
    "    if task_index_remapping_json_file_path is not None:\n",
    "        \n",
    "        with open(task_index_remapping_json_file_path, 'r') as f:\n",
    "            task_index_remapping = json.load(f)\n",
    "            \n",
    "        remapped_task_index = task_index_remapping[task_index]\n",
    "        \n",
    "        if verbose:\n",
    "            log('Remapped task index %d into %d.' % (task_index, remapped_task_index))\n",
    "        \n",
    "        task_index = remapped_task_index\n",
    "    \n",
    "    if 'TOTAL_TASKS' in os.environ:\n",
    "        total_tasks = int(os.getenv('TOTAL_TASKS'))\n",
    "    elif parse_total_tasks_by_max_variable:\n",
    "        total_tasks = int(os.getenv('SLURM_ARRAY_TASK_MAX')) + 1\n",
    "    else:\n",
    "        total_tasks = int(os.getenv('SLURM_ARRAY_TASK_COUNT'))\n",
    "    \n",
    "    if log_ids:\n",
    "        log('Running job %s, task %d of %d.' % (job_id, task_index, total_tasks))\n",
    "    \n",
    "    return job_id, total_tasks, task_index \n",
    "\n",
    "\n",
    "### Liftover ###\n",
    "\n",
    "def liftover_locus(liftover, chrom, pos):\n",
    "    try:\n",
    "            \n",
    "        pos = int(pos)\n",
    "\n",
    "        if not isinstance(chrom, str) or not chrom.startswith('chr'):\n",
    "            chrom = 'chr%s' % chrom\n",
    "\n",
    "        (new_chrom, new_pos, _, _), = liftover.convert_coordinate(chrom, pos)\n",
    "\n",
    "        if new_chrom.startswith('chr'):\n",
    "            new_chrom = new_chrom[3:]\n",
    "\n",
    "        return new_chrom, new_pos\n",
    "    except:\n",
    "        return np.nan, np.nan\n",
    "\n",
    "def liftover_loci_in_df(df, chrom_column = 'chromosome', pos_column = 'position', source_ref_genome = 'hg38', \\\n",
    "        target_ref_genome = 'hg19'):\n",
    "    \n",
    "    from pyliftover import LiftOver\n",
    "    \n",
    "    liftover = LiftOver(source_ref_genome, target_ref_genome)\n",
    "    new_loci = []\n",
    "    \n",
    "    for _, (chrom, pos) in df[[chrom_column, pos_column]].iterrows():\n",
    "        new_loci.append(liftover_locus(liftover, chrom, pos))\n",
    "            \n",
    "    new_chroms, new_positions = (pd.Series(list(values), index = df.index) for values in zip(*new_loci))\n",
    "    return pd.concat([new_chroms.rename(chrom_column) if column == chrom_column else (new_positions.rename(pos_column) if \\\n",
    "            column == pos_column else df[column]) for column in df.columns], axis = 1)    \n",
    "    \n",
    "    \n",
    "### Helper classes ###\n",
    "\n",
    "class DummyContext(object):\n",
    "\n",
    "    def __enter__(self):\n",
    "        pass\n",
    "        \n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        pass\n",
    "\n",
    "class TimeMeasure(object):\n",
    "\n",
    "    def __init__(self, opening_statement):\n",
    "        self.opening_statement = opening_statement\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start_time = datetime.now()\n",
    "        log(self.opening_statement)\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "        self.finish_time = datetime.now()\n",
    "        self.elapsed_time = self.finish_time - self.start_time\n",
    "        log('Finished after %s.' % self.elapsed_time)\n",
    "        \n",
    "class Profiler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.creation_time = datetime.now()\n",
    "        self.profiles = defaultdict(Profiler.Profile)\n",
    "        \n",
    "    def measure(self, profile_name):\n",
    "        return self.profiles[profile_name].measure()\n",
    "        \n",
    "    def format(self, delimiter = '\\n'):\n",
    "        all_profiles = list(self.profiles.items()) + [('Total', Profiler.Profile(total_invokes = 1, total_time = datetime.now() - self.creation_time))]\n",
    "        sorted_profiles = sorted(all_profiles, key = lambda profile_tuple: profile_tuple[1].total_time, reverse = True)\n",
    "        return delimiter.join(['%s: %s' % (profile_name, profile) for profile_name, profile in sorted_profiles])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.format()\n",
    "        \n",
    "    class Profile(object):\n",
    "    \n",
    "        def __init__(self, total_invokes = 0, total_time = timedelta(0)):\n",
    "            self.total_invokes = total_invokes\n",
    "            self.total_time = total_time\n",
    "            \n",
    "        def measure(self):\n",
    "            return Profiler._Measurement(self)\n",
    "            \n",
    "        def __repr__(self):\n",
    "            return '%s (%d times)' % (self.total_time, self.total_invokes)\n",
    "        \n",
    "    class _Measurement(object):\n",
    "    \n",
    "        def __init__(self, profile):\n",
    "            self.profile = profile\n",
    "        \n",
    "        def __enter__(self):\n",
    "            self.start_time = datetime.now()\n",
    "            \n",
    "        def __exit__(self, exc_type, exc_value, exc_traceback):\n",
    "            self.profile.total_time += (datetime.now() - self.start_time)\n",
    "            self.profile.total_invokes += 1\n",
    "\n",
    "\n",
    "class OutputType:\n",
    "    \n",
    "    def __init__(self, is_seq, output_type):\n",
    "        self.is_seq = is_seq\n",
    "        self.output_type = output_type\n",
    "        self.is_numeric = (output_type == 'numeric')\n",
    "        self.is_binary = (output_type == 'binary')\n",
    "        self.is_categorical = (output_type == 'categorical')\n",
    "        \n",
    "    def __str__(self):\n",
    "        if self.is_seq:\n",
    "            return '%s sequence' % self.output_type\n",
    "        else:\n",
    "            return 'global %s' % self.output_type\n",
    "            \n",
    "class OutputSpec:\n",
    "\n",
    "    def __init__(self, output_type, unique_labels = None):\n",
    "        \n",
    "        if output_type.is_numeric:\n",
    "            assert unique_labels is None\n",
    "        elif output_type.is_binary:\n",
    "            if unique_labels is None:\n",
    "                unique_labels = [0, 1]\n",
    "            else:\n",
    "                assert unique_labels == [0, 1]\n",
    "        elif output_type.is_categorical:\n",
    "            assert unique_labels is not None\n",
    "        else:\n",
    "            raise ValueError('Unexpected output type: %s' % output_type)\n",
    "        \n",
    "        self.output_type = output_type\n",
    "        self.unique_labels = unique_labels\n",
    "        \n",
    "        if unique_labels is not None:\n",
    "            self.n_unique_labels = len(unique_labels)\n",
    "            \n",
    "def finetune(model_generator, input_encoder, output_spec, train_seqs, train_raw_Y, valid_seqs = None, valid_raw_Y = None, seq_len = 512, batch_size = 32, \\\n",
    "        max_epochs_per_stage = 40, lr = None, begin_with_frozen_pretrained_layers = True, lr_with_frozen_pretrained_layers = None, n_final_epochs = 1, \\\n",
    "        final_seq_len = 1024, final_lr = None, callbacks = []):\n",
    "        \n",
    "    encoded_train_set, encoded_valid_set = encode_train_and_valid_sets(train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, input_encoder, output_spec, seq_len)\n",
    "        \n",
    "    if begin_with_frozen_pretrained_layers:\n",
    "        log('Training with frozen pretrained layers...')\n",
    "        model_generator.train(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr = lr_with_frozen_pretrained_layers, \\\n",
    "                callbacks = callbacks, freeze_pretrained_layers = True)\n",
    "     \n",
    "    log('Training the entire fine-tuned model...')\n",
    "    model_generator.train(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr = lr, callbacks = callbacks, \\\n",
    "            freeze_pretrained_layers = False)\n",
    "                \n",
    "    if n_final_epochs > 0:\n",
    "        log('Training on final epochs of sequence length %d...' % final_seq_len)\n",
    "        final_batch_size = max(int(batch_size / (final_seq_len / seq_len)), 1)\n",
    "        encoded_train_set, encoded_valid_set = encode_train_and_valid_sets(train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, input_encoder, output_spec, final_seq_len)\n",
    "        model_generator.train(encoded_train_set, encoded_valid_set, final_seq_len, final_batch_size, n_final_epochs, lr = final_lr, callbacks = callbacks, \\\n",
    "                freeze_pretrained_layers = False)\n",
    "                \n",
    "    model_generator.optimizer_weights = None\n",
    "\n",
    "def evaluate_by_len(model_generator, input_encoder, output_spec, seqs, raw_Y, start_seq_len = 512, start_batch_size = 32, increase_factor = 2):\n",
    "    \n",
    "    assert model_generator.optimizer_weights is None\n",
    "    \n",
    "    dataset = pd.DataFrame({'seq': seqs, 'raw_y': raw_Y})\n",
    "        \n",
    "    results = []\n",
    "    results_names = []\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    \n",
    "    for len_matching_dataset, seq_len, batch_size in split_dataset_by_len(dataset, start_seq_len = start_seq_len, start_batch_size = start_batch_size, \\\n",
    "            increase_factor = increase_factor):\n",
    "\n",
    "        X, y_true, sample_weights = encode_dataset(len_matching_dataset['seq'], len_matching_dataset['raw_y'], input_encoder, output_spec, \\\n",
    "                seq_len = seq_len, needs_filtering = False)\n",
    "        \n",
    "        assert set(np.unique(sample_weights)) <= {0.0, 1.0}\n",
    "        y_mask = (sample_weights == 1)\n",
    "        \n",
    "        model = model_generator.create_model(seq_len)\n",
    "        y_pred = model.predict(X, batch_size = batch_size)\n",
    "        \n",
    "        y_true = y_true[y_mask].flatten()\n",
    "        y_pred = y_pred[y_mask]\n",
    "        \n",
    "        if output_spec.output_type.is_categorical:\n",
    "            y_pred = y_pred.reshape((-1, y_pred.shape[-1]))\n",
    "        else:\n",
    "            y_pred = y_pred.flatten()\n",
    "        \n",
    "        results.append(get_evaluation_results(y_true, y_pred, output_spec))\n",
    "        results_names.append(seq_len)\n",
    "        \n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(y_pred)\n",
    "        \n",
    "    y_true = np.concatenate(y_trues, axis = 0)\n",
    "    y_pred = np.concatenate(y_preds, axis = 0)\n",
    "    all_results, confusion_matrix = get_evaluation_results(y_true, y_pred, output_spec, return_confusion_matrix = True)\n",
    "    results.append(all_results)\n",
    "    results_names.append('All')\n",
    "    \n",
    "    results = pd.DataFrame(results, index = results_names)\n",
    "    results.index.name = 'Model seq len'\n",
    "    \n",
    "    return results, confusion_matrix\n",
    "\n",
    "def get_evaluation_results(y_true, y_pred, output_spec, return_confusion_matrix = False):\n",
    "\n",
    "    from scipy.stats import spearmanr\n",
    "    from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "            \n",
    "    results = {}\n",
    "    results['# records'] = len(y_true)\n",
    "            \n",
    "    if output_spec.output_type.is_numeric:\n",
    "        results['Spearman\\'s rank correlation'] = spearmanr(y_true, y_pred)[0]\n",
    "        confusion_matrix = None\n",
    "    else:\n",
    "    \n",
    "        str_unique_labels = list(map(str, output_spec.unique_labels))\n",
    "        \n",
    "        if output_spec.output_type.is_binary:\n",
    "            \n",
    "            y_pred_classes = (y_pred >= 0.5)\n",
    "            \n",
    "            if len(np.unique(y_true)) == 2:\n",
    "                results['AUC'] = roc_auc_score(y_true, y_pred)\n",
    "            else:\n",
    "                results['AUC'] = np.nan\n",
    "        elif output_spec.output_type.is_categorical:\n",
    "            y_pred_classes = y_pred.argmax(axis = -1)\n",
    "            results['Accuracy'] = accuracy_score(y_true, y_pred_classes)\n",
    "        else:\n",
    "            raise ValueError('Unexpected output type: %s' % output_spec.output_type)\n",
    "                    \n",
    "        confusion_matrix = pd.DataFrame(confusion_matrix(y_true, y_pred_classes, labels = np.arange(output_spec.n_unique_labels)), index = str_unique_labels, \\\n",
    "                    columns = str_unique_labels)\n",
    "         \n",
    "    if return_confusion_matrix:\n",
    "        return results, confusion_matrix\n",
    "    else:\n",
    "        return results\n",
    "        \n",
    "def encode_train_and_valid_sets(train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, input_encoder, output_spec, seq_len):\n",
    "    \n",
    "    encoded_train_set = encode_dataset(train_seqs, train_raw_Y, input_encoder, output_spec, seq_len = seq_len, needs_filtering = True, \\\n",
    "            dataset_name = 'Training set')\n",
    "    \n",
    "    if valid_seqs is None and valid_raw_Y is None:\n",
    "        encoded_valid_set = None\n",
    "    else:\n",
    "        encoded_valid_set = encode_dataset(valid_seqs, valid_raw_Y, input_encoder, output_spec, seq_len = seq_len, needs_filtering = True, \\\n",
    "                dataset_name = 'Validation set')\n",
    "\n",
    "    return encoded_train_set, encoded_valid_set\n",
    "        \n",
    "def encode_dataset(seqs, raw_Y, input_encoder, output_spec, seq_len = 512, needs_filtering = True, dataset_name = 'Dataset', verbose = True):\n",
    "    \n",
    "    if needs_filtering:\n",
    "        dataset = pd.DataFrame({'seq': seqs, 'raw_Y': raw_Y})\n",
    "        dataset = filter_dataset_by_len(dataset, seq_len = seq_len, dataset_name = dataset_name, verbose = verbose)\n",
    "        seqs = dataset['seq']\n",
    "        raw_Y = dataset['raw_Y']\n",
    "    \n",
    "    X = input_encoder.encode_X(seqs, seq_len)\n",
    "    Y, sample_weigths = encode_Y(raw_Y, output_spec, seq_len = seq_len)\n",
    "    return X, Y, sample_weigths\n",
    "\n",
    "def encode_Y(raw_Y, output_spec, seq_len = 512):\n",
    "    if output_spec.output_type.is_seq:\n",
    "        return encode_seq_Y(raw_Y, seq_len, output_spec.output_type.is_binary, output_spec.unique_labels)\n",
    "    elif output_spec.output_type.is_categorical:\n",
    "        return encode_categorical_Y(raw_Y, output_spec.unique_labels), np.ones(len(raw_Y))\n",
    "    elif output_spec.output_type.is_numeric or output_spec.output_type.is_binary:\n",
    "        return raw_Y.values.astype(float), np.ones(len(raw_Y))\n",
    "    else:\n",
    "        raise ValueError('Unexpected output type: %s' % output_spec.output_type)\n",
    "\n",
    "def encode_seq_Y(seqs, seq_len, is_binary, unique_labels):\n",
    "\n",
    "    label_to_index = {str(label): i for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    Y = np.zeros((len(seqs), seq_len), dtype = int)\n",
    "    sample_weigths = np.zeros((len(seqs), seq_len))\n",
    "    \n",
    "    for i, seq in enumerate(seqs):\n",
    "        \n",
    "        for j, label in enumerate(seq):\n",
    "            # +1 to account for the <START> token at the beginning.\n",
    "            \n",
    "            Y[i, j + 1] = label_to_index[label]\n",
    "            \n",
    "        sample_weigths[i, 1:(len(seq) + 1)] = 1\n",
    "        \n",
    "    if is_binary:\n",
    "        Y = np.expand_dims(Y, axis = -1)\n",
    "        sample_weigths = np.expand_dims(sample_weigths, axis = -1)\n",
    "    \n",
    "    return Y, sample_weigths\n",
    "    \n",
    "def encode_categorical_Y(labels, unique_labels):\n",
    "    \n",
    "    label_to_index = {label: i for i, label in enumerate(unique_labels)}\n",
    "    Y = np.zeros(len(labels), dtype = int)\n",
    "    \n",
    "    for i, label in enumerate(labels):\n",
    "        Y[i] = label_to_index[label]\n",
    "        \n",
    "    return Y\n",
    "    \n",
    "def filter_dataset_by_len(dataset, seq_len = 512, seq_col_name = 'seq', dataset_name = 'Dataset', verbose = True):\n",
    "    \n",
    "    max_allowed_input_seq_len = seq_len - ADDED_TOKENS_PER_SEQ\n",
    "    filtered_dataset = dataset[dataset[seq_col_name].str.len() <= max_allowed_input_seq_len]\n",
    "    n_removed_records = len(dataset) - len(filtered_dataset)\n",
    "    \n",
    "    if verbose:\n",
    "        log('%s: Filtered out %d of %d (%.1f%%) records of lengths exceeding %d.' % (dataset_name, n_removed_records, len(dataset), 100 * n_removed_records / len(dataset), \\\n",
    "                max_allowed_input_seq_len))\n",
    "    \n",
    "    return filtered_dataset\n",
    "    \n",
    "def split_dataset_by_len(dataset, seq_col_name = 'seq', start_seq_len = 512, start_batch_size = 32, increase_factor = 2):\n",
    "\n",
    "    seq_len = start_seq_len\n",
    "    batch_size = start_batch_size\n",
    "    \n",
    "    while len(dataset) > 0:\n",
    "        max_allowed_input_seq_len = seq_len - ADDED_TOKENS_PER_SEQ\n",
    "        len_mask = (dataset[seq_col_name].str.len() <= max_allowed_input_seq_len)\n",
    "        len_matching_dataset = dataset[len_mask]\n",
    "        yield len_matching_dataset, seq_len, batch_size\n",
    "        dataset = dataset[~len_mask]\n",
    "        seq_len *= increase_factor\n",
    "        batch_size = max(batch_size // increase_factor, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a82bb3-1e84-43b3-8909-902e10fa4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load your dataset\n",
    "# Replace these file paths with the paths to your own training and testing data\n",
    "TRAIN_DATA_PATH = 'train_protein.csv'\n",
    "TEST_DATA_PATH = 'test_protein.csv'\n",
    "\n",
    "# Load the dataset\n",
    "train_set = pd.read_csv(TRAIN_DATA_PATH).dropna().drop_duplicates()\n",
    "test_set = pd.read_csv(TEST_DATA_PATH).dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a4146-3950-4597-b400-f0aff76dacaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110147c6-36a3-4001-98a0-be1201c658fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e81f94-a676-46fb-bbb4-b78fa2e16b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Split the training set into train and validation sets\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca1609-ac5b-4fcd-9b7a-a4bdc4477ce0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_set['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab2a51-29ad-4fc1-863b-33a8dabc73bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract unique labels from your dataset\n",
    "all_labels = set()\n",
    "for seq in train_set['label']:\n",
    "    all_labels.update(seq)\n",
    "for seq in valid_set['label']:\n",
    "    all_labels.update(seq)\n",
    "\n",
    "# Convert the set of all labels to a sorted list\n",
    "UNIQUE_LABELS = sorted(list(all_labels))\n",
    "\n",
    "# Update the OUTPUT_SPEC with the correct labels\n",
    "OUTPUT_SPEC = OutputSpec(OutputType(True, 'categorical'), UNIQUE_LABELS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2207388-7568-4046-af1c-becce71dfd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Output Specification\n",
    "# Customize this based on your dataset's labels\n",
    "# Example for binary classification\n",
    "# Set output type as text\n",
    "\n",
    "label_list = [x for x in train_set['label']]\n",
    "\n",
    "\n",
    "all_labels = set()\n",
    "for seq in train_set['label']:\n",
    "    all_labels.update(seq)\n",
    "for seq in valid_set['label']:\n",
    "    all_labels.update(seq)\n",
    "    \n",
    "label_list.append(all_labels)\n",
    "\n",
    "\n",
    "OUTPUT_TYPE = OutputType(True, 'categorical')\n",
    "\n",
    "# Define unique labels as needed\n",
    "UNIQUE_LABELS = label_list  # Adjust these to match your dataset's labels\n",
    "\n",
    "# Create the output specification\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15554c65-1b06-4727-9748-f54660162642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Pre-trained ProteinBERT Model\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "# Step 4: Create the Fine-tuning Model\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, \n",
    "    pretraining_model_manipulation_function=get_model_with_hidden_layers_as_outputs, dropout_rate=0.5)\n",
    "\n",
    "# Step 5: Set up Callbacks for Training\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.25, min_lr=1e-05, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Step 6: Fine-tune the Model\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \n",
    "    seq_len=512, batch_size=32, max_epochs_per_stage=1, lr=1e-04, begin_with_frozen_pretrained_layers=True, \n",
    "    lr_with_frozen_pretrained_layers=1e-02, n_final_epochs=1, final_seq_len=1024, final_lr=1e-05, callbacks=training_callbacks)\n",
    "\n",
    "# Step 7: Evaluate the Model on the Test Set\n",
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set['seq'], test_set['label'], \n",
    "    start_seq_len=512, start_batch_size=32)\n",
    "\n",
    "print('Test-set performance:')\n",
    "print(results)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177b906a-e02f-49af-8bd7-be1d70f17e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee67defc-6f7c-4534-8aaa-63abc8272222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example: Predict on new data\n",
    "# Replace 'new_sequences' with your list of sequences to predict on\n",
    "new_sequences = ['MVLSPADKTNVKAAW', 'GVLTQSQAELERVH']\n",
    "\n",
    "# If your data is in a DataFrame:\n",
    "# new_data = pd.DataFrame({'seq': ['MVLSPADKTNVKAAW', 'GVLTQSQAELERVH']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb4efd5-e729-4f3c-9193-02fc14591788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from proteinbert import predict\n",
    "\n",
    "# Predictions (if you have a DataFrame or list of sequences)\n",
    "predictions = predict(model_generator, input_encoder, OUTPUT_SPEC, new_sequences, seq_len=512, batch_size=32)\n",
    "\n",
    "# Output the predictions\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf9837-e748-402d-9616-6d588a19d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert os.path.exists(TRAIN_DATA_PATH), f\"{TRAIN_DATA_PATH} does not exist.\"\n",
    "assert os.path.exists(TEST_DATA_PATH), f\"{TEST_DATA_PATH} does not exist.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d75335-36bb-49b5-a141-15e38ed7585a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_set.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698b45e8-ef6a-4f06-b8ff-9521a49a9043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(test_set.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce6f836-5087-4ca4-92fe-775dd9bc2030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "print(train_set.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68cb7d-0e5c-4409-8a06-18469caba25f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c0539-47ee-4429-a319-592b4c74e5ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Unique labels: {UNIQUE_LABELS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e2165cb-264c-4885-b6c8-bad3ddee18d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f804ce1-c455-4f20-9add-ff4e6970b4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Load your dataset\n",
    "TRAIN_DATA_PATH = 'train_protein.csv'\n",
    "TEST_DATA_PATH = 'test_protein.csv'\n",
    "\n",
    "# Load the dataset\n",
    "train_set = pd.read_csv(TRAIN_DATA_PATH).dropna().drop_duplicates()\n",
    "test_set = pd.read_csv(TEST_DATA_PATH).dropna().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d702dbbf-928f-4ae4-ac70-039041ce7a71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2563 training set records, 641 validation set records, 1375 test set records.\n"
     ]
    }
   ],
   "source": [
    "# Split the training set into train and validation sets\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "062487f6-3209-48e7-b82f-719f39859201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract unique labels from your dataset\n",
    "all_labels = set()\n",
    "for label in train_set['label']:\n",
    "    all_labels.add(label)\n",
    "for label in valid_set['label']:\n",
    "    all_labels.add(label)\n",
    "\n",
    "# Convert the set of all labels to a sorted list\n",
    "UNIQUE_LABELS = sorted(list(all_labels))\n",
    "\n",
    "# Update the OUTPUT_SPEC with the correct labels\n",
    "OUTPUT_SPEC = OutputSpec(OutputType(False, 'categorical'), UNIQUE_LABELS)\n",
    "# False is for isBinary\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb2932a0-03d6-4dc0-b8db-5a0866e25dd1",
   "metadata": {
    "tags": []
   },
   "source": [
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "print(pretrained_model_generator)\n",
    "print(input_encoder)\n",
    "\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, \n",
    "    pretraining_model_manipulation_function=get_model_with_hidden_layers_as_outputs, dropout_rate=0.5)\n",
    "print(model_generator)\n",
    "\n",
    "# Step 5: Set up Callbacks for Training\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.25, min_lr=1e-05, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0e83efe-d4cc-4c85-afb8-37304e0986eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, \n",
    "    train_set['seq'], train_set['label'], \n",
    "    valid_set['seq'], valid_set['label'], \n",
    "    seq_len=128, batch_size=8, max_epochs_per_stage=1, \n",
    "    lr=1e-04, begin_with_frozen_pretrained_layers=True, \n",
    "    lr_with_frozen_pretrained_layers=1e-02, \n",
    "    n_final_epochs=1, final_seq_len=256, \n",
    "    final_lr=1e-05, callbacks=training_callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9e5fd3f-40cc-463f-a082-70f0cf7210a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024_08_09-00:40:07] Training set: Filtered out 32 of 2563 (1.2%) records of lengths exceeding 1022.\n",
      "[2024_08_09-00:40:07] Validation set: Filtered out 11 of 641 (1.7%) records of lengths exceeding 1022.\n",
      "[2024_08_09-00:40:07] Training with frozen pretrained layers...\n",
      "145\n",
      "\u001b[1m80/80\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 754ms/step - loss: 8.1408 - val_loss: 8.5370 - learning_rate: 2.0000e-04\n",
      "[2024_08_09-00:41:13] Training the entire fine-tuned model...\n",
      "146\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You called `set_weights(weights)` on layer 'functional_2' with a weight list of length 146, but the layer was expecting 145 weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m      9\u001b[0m training_callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     10\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m     keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     12\u001b[0m ]\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Step 6: Fine-tune the Model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m'\u001b[39m], train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], valid_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m'\u001b[39m], valid_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     16\u001b[0m     seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, max_epochs_per_stage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-05\u001b[39m, begin_with_frozen_pretrained_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     17\u001b[0m     lr_with_frozen_pretrained_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-02\u001b[39m, n_final_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, final_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, final_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-05\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mtraining_callbacks)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Step 7: Evaluate the Model on the Test Set\u001b[39;00m\n\u001b[1;32m     20\u001b[0m results, confusion_matrix \u001b[38;5;241m=\u001b[39m evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq\u001b[39m\u001b[38;5;124m'\u001b[39m], test_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     21\u001b[0m     start_seq_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, start_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/SEM 2/Thesis/Data/protein_bert-master/venv/proteinbert/finetuning.py:56\u001b[0m, in \u001b[0;36mfinetune\u001b[0;34m(model_generator, input_encoder, output_spec, train_seqs, train_raw_Y, valid_seqs, valid_raw_Y, seq_len, batch_size, max_epochs_per_stage, lr, begin_with_frozen_pretrained_layers, lr_with_frozen_pretrained_layers, n_final_epochs, final_seq_len, final_lr, callbacks)\u001b[0m\n\u001b[1;32m     52\u001b[0m     model_generator\u001b[38;5;241m.\u001b[39mtrain(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr \u001b[38;5;241m=\u001b[39m lr_with_frozen_pretrained_layers, \\\n\u001b[1;32m     53\u001b[0m             callbacks \u001b[38;5;241m=\u001b[39m callbacks, freeze_pretrained_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     55\u001b[0m log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining the entire fine-tuned model...\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m model_generator\u001b[38;5;241m.\u001b[39mtrain(encoded_train_set, encoded_valid_set, seq_len, batch_size, max_epochs_per_stage, lr \u001b[38;5;241m=\u001b[39m lr, callbacks \u001b[38;5;241m=\u001b[39m callbacks, \\\n\u001b[1;32m     57\u001b[0m         freeze_pretrained_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_final_epochs \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     60\u001b[0m     log(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining on final epochs of sequence length \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m final_seq_len)\n",
      "File \u001b[0;32m~/Desktop/SEM 2/Thesis/Data/protein_bert-master/venv/proteinbert/model_generation.py:24\u001b[0m, in \u001b[0;36mModelGenerator.train\u001b[0;34m(self, encoded_train_set, encoded_valid_set, seq_len, batch_size, n_epochs, lr, callbacks, **create_model_kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m train_X, train_Y, train_sample_weigths \u001b[38;5;241m=\u001b[39m encoded_train_set\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdummy_epoch \u001b[38;5;241m=\u001b[39m (_slice_arrays(train_X, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)), _slice_arrays(train_Y, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 24\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_model(seq_len, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcreate_model_kwargs)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m lr\n",
      "File \u001b[0;32m~/Desktop/SEM 2/Thesis/Data/protein_bert-master/venv/proteinbert/model_generation.py:145\u001b[0m, in \u001b[0;36mFinetuningModelGenerator.create_model\u001b[0;34m(self, seq_len, freeze_pretrained_layers)\u001b[0m\n\u001b[1;32m    142\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(inputs \u001b[38;5;241m=\u001b[39m model_inputs, outputs \u001b[38;5;241m=\u001b[39m output_layer)\n\u001b[1;32m    143\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss \u001b[38;5;241m=\u001b[39m loss, optimizer \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_class(learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mother_optimizer_kwargs))\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_weights(model)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/Desktop/SEM 2/Thesis/Data/protein_bert-master/venv/proteinbert/model_generation.py:50\u001b[0m, in \u001b[0;36mModelGenerator._init_weights\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(copy_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_weights)))\n\u001b[0;32m---> 50\u001b[0m     model\u001b[38;5;241m.\u001b[39mset_weights(copy_weights(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_weights))\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer_weights) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mvariables()):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/layers/layer.py:677\u001b[0m, in \u001b[0;36mLayer.set_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    675\u001b[0m layer_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_weights) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(weights):\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    678\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou called `set_weights(weights)` on layer \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    679\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a weight list of length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwas expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_weights)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m weights.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    681\u001b[0m     )\n\u001b[1;32m    682\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(layer_weights, weights):\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m variable\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m value\u001b[38;5;241m.\u001b[39mshape:\n",
      "\u001b[0;31mValueError\u001b[0m: You called `set_weights(weights)` on layer 'functional_2' with a weight list of length 146, but the layer was expecting 145 weights."
     ]
    }
   ],
   "source": [
    "# Step 3: Load Pre-trained ProteinBERT Model\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "# Step 4: Create the Fine-tuning Model\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, \n",
    "    pretraining_model_manipulation_function=get_model_with_hidden_layers_as_outputs, dropout_rate=0.5)\n",
    "\n",
    "# Step 5: Set up Callbacks for Training\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=1, factor=0.25, min_lr=1e-05, verbose=1),\n",
    "    keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Step 6: Fine-tune the Model\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \n",
    "    seq_len=1024, batch_size=32, max_epochs_per_stage=1, lr=1e-04, begin_with_frozen_pretrained_layers=True, \n",
    "    lr_with_frozen_pretrained_layers=1e-02, n_final_epochs=1, final_seq_len=1024, final_lr=1e-05, callbacks=training_callbacks)\n",
    "\n",
    "# Step 7: Evaluate the Model on the Test Set\n",
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, test_set['seq'], test_set['label'], \n",
    "    start_seq_len=1024, start_batch_size=32)\n",
    "\n",
    "print('Test-set performance:')\n",
    "print(results)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1332c688-bfef-417a-ab72-c93a6baa5a89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbe62055-d758-435d-b3b6-20deab19f2d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y contains previously unseen labels: 'YJAA_ECOLI'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:225\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m_map_to_integer\u001b[0;34m(values, uniques)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:165\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    164\u001b[0m table \u001b[38;5;241m=\u001b[39m _nandict({val: i \u001b[38;5;28;01mfor\u001b[39;00m i, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(uniques)})\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([table[v] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:159\u001b[0m, in \u001b[0;36m_nandict.__missing__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_value\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'YJAA_ECOLI'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m     17\u001b[0m train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(train_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 18\u001b[0m valid_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(valid_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     19\u001b[0m test_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mtransform(test_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:137\u001b[0m, in \u001b[0;36mLabelEncoder.transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _num_samples(y) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([])\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _encode(y, uniques\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_encode.py:227\u001b[0m, in \u001b[0;36m_encode\u001b[0;34m(values, uniques, check_unknown)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _map_to_integer(values, uniques)\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 227\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my contains previously unseen labels: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check_unknown:\n",
      "\u001b[0;31mValueError\u001b[0m: y contains previously unseen labels: 'YJAA_ECOLI'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "TRAIN_DATA_PATH = 'train_protein.csv'\n",
    "TEST_DATA_PATH = 'test_protein.csv'\n",
    "\n",
    "train_set = pd.read_csv(TRAIN_DATA_PATH).dropna().drop_duplicates()\n",
    "test_set = pd.read_csv(TEST_DATA_PATH).dropna().drop_duplicates()\n",
    "\n",
    "# Split the training set into train and validation sets\n",
    "train_set, valid_set = train_test_split(train_set, test_size=0.2, random_state=42)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_set['label'] = label_encoder.fit_transform(train_set['label'])\n",
    "valid_set['label'] = label_encoder.transform(valid_set['label'])\n",
    "test_set['label'] = label_encoder.transform(test_set['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ebee5c-3706-41eb-a02e-87e2ca97f0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (shrutik_thesis)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
